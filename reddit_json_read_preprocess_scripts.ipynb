{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Risha\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "#must pip install textstat\n",
    "# import textstat\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import gensim\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by reading in reddit json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>can_gild</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>edited</th>\n",
       "      <th>gilded</th>\n",
       "      <th>...</th>\n",
       "      <th>num_user_mentions</th>\n",
       "      <th>num_subreddit_mentions</th>\n",
       "      <th>has_flair</th>\n",
       "      <th>num_outside_links</th>\n",
       "      <th>num_reddit_links</th>\n",
       "      <th>title_is_question</th>\n",
       "      <th>num_questions</th>\n",
       "      <th>readability</th>\n",
       "      <th>uppercase_ratio</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouthfulPhotographer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Welcome to generation void</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1517443200</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jasonklacour</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Welcome</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1517443200</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Assassin2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm 16 and the friend told me he was joking af...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1517443200</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kawaiicicle</td>\n",
       "      <td>Employee</td>\n",
       "      <td>Assistant Manager</td>\n",
       "      <td>What? \\r It’s a niche rpg. Most rpg fans are a...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1517443200</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recklessmaterialism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>solid!</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1517443200</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author author_flair_css_class  author_flair_text  \\\n",
       "0  YouthfulPhotographer                    NaN                NaN   \n",
       "1          jasonklacour                    NaN                NaN   \n",
       "2          Assassin2000                    NaN                NaN   \n",
       "3           kawaiicicle               Employee  Assistant Manager   \n",
       "4   recklessmaterialism                    NaN                NaN   \n",
       "\n",
       "                                                body  can_gild  \\\n",
       "0                        Welcome to generation void       True   \n",
       "1                                            Welcome      True   \n",
       "2  I'm 16 and the friend told me he was joking af...      True   \n",
       "3  What? \\r It’s a niche rpg. Most rpg fans are a...      True   \n",
       "4                                             solid!      True   \n",
       "\n",
       "   controversiality  created_utc distinguished  edited  gilded    ...     \\\n",
       "0               0.0   1517443200         False   False       0    ...      \n",
       "1               0.0   1517443200         False   False       0    ...      \n",
       "2               0.0   1517443200         False   False       0    ...      \n",
       "3               0.0   1517443200         False   False       0    ...      \n",
       "4               0.0   1517443200         False   False       0    ...      \n",
       "\n",
       "   num_user_mentions num_subreddit_mentions  has_flair  num_outside_links  \\\n",
       "0                  0                      0      False                  0   \n",
       "1                  0                      0      False                  0   \n",
       "2                  0                      0      False                  0   \n",
       "3                  0                      0       True                  0   \n",
       "4                  0                      0      False                  0   \n",
       "\n",
       "  num_reddit_links title_is_question  num_questions  readability  \\\n",
       "0                0                 0              0         13.1   \n",
       "1                0                 0              0          8.4   \n",
       "2                0                 0              0          6.4   \n",
       "3                0                 1              2          2.9   \n",
       "4                0                 0              0          8.4   \n",
       "\n",
       "   uppercase_ratio  length\\r  \n",
       "0         0.037037        27  \n",
       "1         0.142857         7  \n",
       "2         0.031250        64  \n",
       "3         0.044643       112  \n",
       "4         0.000000         6  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open('/Users/jayshenoy/Documents/DataScienceLabs/Final_Project/data_checkpoint.csv') as json_file:      \n",
    "#     data = json_file.readlines()\n",
    "#     # this line below may take at least 8-10 minutes of processing for 4-5 million rows. It converts all strings in list to actual json objects. \n",
    "#     data = list(map(json.loads, data)) \n",
    "# frame=pd.DataFrame(data)\n",
    "# frame.head()\n",
    "frame=pd.read_csv('engineered_reddit_train.csv', lineterminator='\\n')\n",
    "frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2885987"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_bodies_df(df):\n",
    "    df['body'] = pd.Series([body.replace(\"\\n\", \" \") for body in df['body']])\n",
    "    return df\n",
    "\n",
    "cleanedframe=clean_bodies_df(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "cleanedframe=frame\n",
    "print(cleanedframe['body'].isna().sum())\n",
    "print(cleanedframe['body'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "50000\n",
      "75000\n",
      "100000\n",
      "125000\n",
      "150000\n",
      "175000\n",
      "200000\n",
      "225000\n",
      "250000\n",
      "275000\n",
      "300000\n",
      "325000\n",
      "350000\n",
      "375000\n",
      "400000\n",
      "425000\n",
      "450000\n",
      "475000\n",
      "500000\n",
      "525000\n",
      "550000\n",
      "575000\n",
      "600000\n",
      "625000\n",
      "650000\n",
      "675000\n",
      "700000\n",
      "725000\n",
      "750000\n",
      "775000\n",
      "800000\n",
      "825000\n",
      "850000\n",
      "875000\n",
      "900000\n",
      "925000\n",
      "950000\n",
      "975000\n",
      "1000000\n",
      "1025000\n",
      "1050000\n",
      "1075000\n",
      "1100000\n",
      "1125000\n",
      "1150000\n",
      "1175000\n",
      "1200000\n",
      "1225000\n",
      "1250000\n",
      "1275000\n",
      "1300000\n",
      "1325000\n",
      "1350000\n",
      "1375000\n",
      "1400000\n",
      "1425000\n",
      "1450000\n",
      "1475000\n",
      "1500000\n",
      "1525000\n",
      "1550000\n",
      "1575000\n",
      "1600000\n",
      "1625000\n",
      "1650000\n",
      "1675000\n",
      "1700000\n",
      "1725000\n",
      "1750000\n",
      "1775000\n",
      "1800000\n",
      "1825000\n",
      "1850000\n",
      "1875000\n",
      "1900000\n",
      "1925000\n",
      "1950000\n",
      "1975000\n",
      "2000000\n",
      "2025000\n",
      "2050000\n",
      "2075000\n",
      "2100000\n",
      "2125000\n",
      "2150000\n",
      "2175000\n",
      "2200000\n",
      "2225000\n",
      "2250000\n",
      "2275000\n",
      "2300000\n",
      "2325000\n",
      "2350000\n",
      "2375000\n",
      "2400000\n",
      "2425000\n",
      "2450000\n",
      "2475000\n",
      "2500000\n",
      "2525000\n",
      "2550000\n",
      "2575000\n",
      "2600000\n",
      "2625000\n",
      "2650000\n",
      "2675000\n",
      "2700000\n",
      "2725000\n",
      "2750000\n",
      "2775000\n",
      "2800000\n",
      "2825000\n",
      "2850000\n",
      "2875000\n"
     ]
    }
   ],
   "source": [
    "#tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(cleanedframe['body'])]\n",
    "tagged_data=[]\n",
    "c=0\n",
    "for i, _d in enumerate(cleanedframe['body']):\n",
    "    tagged_data.append(TaggedDocument(words=word_tokenize(_d.lower()),tags=[str(i)]))\n",
    "    c+=1\n",
    "    if c%25000==0:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Risha\\Documents\\Data Science Lab\\Final Project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_corpus, common_texts, get_tmpfile\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "class EpochSaver(CallbackAny2Vec):\n",
    "\n",
    "    def __init__(self, path_prefix):\n",
    "        self.path_prefix = path_prefix\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "#         output_path = get_tmpfile('{}_epoch{}.model'.format(self.path_prefix, self.epoch))\n",
    "#         model.save(output_path)\n",
    "        model.save('newest_small_d2v_model.model')\n",
    "        self.epoch += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger(CallbackAny2Vec):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "        print(datetime.datetime.now())\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{} end\".format(self.epoch))\n",
    "        print(datetime.datetime.now())\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import PerplexityMetric\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "\n",
    "# Log the perplexity score at the end of each epoch.\n",
    "perplexity_logger = PerplexityMetric(corpus=common_corpus, logger='shell')\n",
    "lda = LdaModel(common_corpus, id2word=common_dictionary, num_topics=5, callbacks=[perplexity_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Risha\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 300\n",
    "vec_size = 50\n",
    "alpha = 0.01\n",
    "saver=EpochSaver(\"mdlo\")\n",
    "logger=EpochLogger()\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.0001,\n",
    "                min_count=1,\n",
    "                subsaplming=1e-6,\n",
    "                window_size=5,\n",
    "                workers=4,\n",
    "                dm=1,callbacks=[saver,logger])\n",
    "  \n",
    "model.build_vocab(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2885987\n"
     ]
    }
   ],
   "source": [
    "print(len(tagged_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2885987"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(gensim.models.doc2vec.FAST_VERSION > -1)\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Epoch #0 start\n",
      "2018-12-15 23:58:01.261218\n",
      "Epoch #0 end\n",
      "2018-12-16 00:01:25.449231\n",
      "epoch: 1\n",
      "Epoch #1 start\n",
      "2018-12-16 00:01:25.450228\n",
      "Epoch #1 end\n",
      "2018-12-16 00:04:54.423908\n",
      "epoch: 2\n",
      "Epoch #2 start\n",
      "2018-12-16 00:04:54.423908\n",
      "Epoch #2 end\n",
      "2018-12-16 00:08:20.248454\n",
      "epoch: 3\n",
      "Epoch #3 start\n",
      "2018-12-16 00:08:20.248454\n",
      "Epoch #3 end\n",
      "2018-12-16 00:11:44.530644\n",
      "epoch: 4\n",
      "Epoch #4 start\n",
      "2018-12-16 00:11:44.530644\n",
      "Epoch #4 end\n",
      "2018-12-16 00:15:14.218782\n",
      "epoch: 5\n",
      "Epoch #5 start\n",
      "2018-12-16 00:15:14.218782\n",
      "Epoch #5 end\n",
      "2018-12-16 00:18:46.238407\n",
      "epoch: 6\n",
      "Epoch #6 start\n",
      "2018-12-16 00:18:46.238407\n",
      "Epoch #6 end\n",
      "2018-12-16 00:22:09.631607\n",
      "epoch: 7\n",
      "Epoch #7 start\n",
      "2018-12-16 00:22:09.631607\n",
      "Epoch #7 end\n",
      "2018-12-16 00:25:34.154058\n",
      "epoch: 8\n",
      "Epoch #8 start\n",
      "2018-12-16 00:25:34.154058\n",
      "Epoch #8 end\n",
      "2018-12-16 00:28:59.284395\n",
      "epoch: 9\n",
      "Epoch #9 start\n",
      "2018-12-16 00:28:59.284395\n",
      "Epoch #9 end\n",
      "2018-12-16 00:32:23.531510\n",
      "epoch: 10\n",
      "Epoch #10 start\n",
      "2018-12-16 00:32:23.532506\n",
      "Epoch #10 end\n",
      "2018-12-16 00:35:45.957542\n",
      "epoch: 11\n",
      "Epoch #11 start\n",
      "2018-12-16 00:35:45.957542\n",
      "Epoch #11 end\n",
      "2018-12-16 00:39:10.000597\n",
      "epoch: 12\n",
      "Epoch #12 start\n",
      "2018-12-16 00:39:10.000597\n",
      "Epoch #12 end\n",
      "2018-12-16 00:42:31.712361\n",
      "epoch: 13\n",
      "Epoch #13 start\n",
      "2018-12-16 00:42:31.712361\n",
      "Epoch #13 end\n",
      "2018-12-16 00:45:54.022531\n",
      "epoch: 14\n",
      "Epoch #14 start\n",
      "2018-12-16 00:45:54.022531\n",
      "Epoch #14 end\n",
      "2018-12-16 00:49:18.593955\n",
      "epoch: 15\n",
      "Epoch #15 start\n",
      "2018-12-16 00:49:18.593955\n",
      "Epoch #15 end\n",
      "2018-12-16 00:52:42.315125\n",
      "epoch: 16\n",
      "Epoch #16 start\n",
      "2018-12-16 00:52:42.315125\n",
      "Epoch #16 end\n",
      "2018-12-16 00:56:04.567600\n",
      "epoch: 17\n",
      "Epoch #17 start\n",
      "2018-12-16 00:56:04.567600\n",
      "Epoch #17 end\n",
      "2018-12-16 00:59:28.890572\n",
      "epoch: 18\n",
      "Epoch #18 start\n",
      "2018-12-16 00:59:28.890572\n",
      "Epoch #18 end\n",
      "2018-12-16 01:02:51.682720\n",
      "epoch: 19\n",
      "Epoch #19 start\n",
      "2018-12-16 01:02:51.682720\n",
      "Epoch #19 end\n",
      "2018-12-16 01:06:13.744062\n",
      "epoch: 20\n",
      "Epoch #20 start\n",
      "2018-12-16 01:06:13.744062\n",
      "Epoch #20 end\n",
      "2018-12-16 01:09:38.350175\n",
      "epoch: 21\n",
      "Epoch #21 start\n",
      "2018-12-16 01:09:38.350175\n",
      "Epoch #21 end\n",
      "2018-12-16 01:13:01.172668\n",
      "epoch: 22\n",
      "Epoch #22 start\n",
      "2018-12-16 01:13:01.172668\n",
      "Epoch #22 end\n",
      "2018-12-16 01:16:22.696717\n",
      "epoch: 23\n",
      "Epoch #23 start\n",
      "2018-12-16 01:16:22.696717\n",
      "Epoch #23 end\n",
      "2018-12-16 01:19:45.346549\n",
      "epoch: 24\n",
      "Epoch #24 start\n",
      "2018-12-16 01:19:45.346549\n",
      "Epoch #24 end\n",
      "2018-12-16 01:23:07.137596\n",
      "epoch: 25\n",
      "Epoch #25 start\n",
      "2018-12-16 01:23:07.137596\n",
      "Epoch #25 end\n",
      "2018-12-16 01:26:28.300613\n",
      "epoch: 26\n",
      "Epoch #26 start\n",
      "2018-12-16 01:26:28.300613\n",
      "Epoch #26 end\n",
      "2018-12-16 01:29:52.991708\n",
      "epoch: 27\n",
      "Epoch #27 start\n",
      "2018-12-16 01:29:52.991708\n",
      "Epoch #27 end\n",
      "2018-12-16 01:33:15.237689\n",
      "epoch: 28\n",
      "Epoch #28 start\n",
      "2018-12-16 01:33:15.237689\n",
      "Epoch #28 end\n",
      "2018-12-16 01:36:37.277234\n",
      "epoch: 29\n",
      "Epoch #29 start\n",
      "2018-12-16 01:36:37.277234\n",
      "Epoch #29 end\n",
      "2018-12-16 01:40:00.010054\n",
      "epoch: 30\n",
      "Epoch #30 start\n",
      "2018-12-16 01:40:00.010054\n",
      "Epoch #30 end\n",
      "2018-12-16 01:43:21.450613\n",
      "epoch: 31\n",
      "Epoch #31 start\n",
      "2018-12-16 01:43:21.450613\n",
      "Epoch #31 end\n",
      "2018-12-16 01:46:45.933933\n",
      "epoch: 32\n",
      "Epoch #32 start\n",
      "2018-12-16 01:46:45.934952\n",
      "Epoch #32 end\n",
      "2018-12-16 01:50:14.954201\n",
      "epoch: 33\n",
      "Epoch #33 start\n",
      "2018-12-16 01:50:14.955197\n",
      "Epoch #33 end\n",
      "2018-12-16 01:53:41.231879\n",
      "epoch: 34\n",
      "Epoch #34 start\n",
      "2018-12-16 01:53:41.231879\n",
      "Epoch #34 end\n",
      "2018-12-16 01:57:09.683254\n",
      "epoch: 35\n",
      "Epoch #35 start\n",
      "2018-12-16 01:57:09.684253\n",
      "Epoch #35 end\n",
      "2018-12-16 02:00:44.735423\n",
      "epoch: 36\n",
      "Epoch #36 start\n",
      "2018-12-16 02:00:44.735423\n",
      "Epoch #36 end\n",
      "2018-12-16 02:04:35.589417\n",
      "epoch: 37\n",
      "Epoch #37 start\n",
      "2018-12-16 02:04:35.589417\n",
      "Epoch #37 end\n",
      "2018-12-16 02:08:32.701652\n",
      "epoch: 38\n",
      "Epoch #38 start\n",
      "2018-12-16 02:08:32.701652\n",
      "Epoch #38 end\n",
      "2018-12-16 02:12:30.283961\n",
      "epoch: 39\n",
      "Epoch #39 start\n",
      "2018-12-16 02:12:30.284958\n",
      "Epoch #39 end\n",
      "2018-12-16 02:17:02.365577\n",
      "epoch: 40\n",
      "Epoch #40 start\n",
      "2018-12-16 02:17:02.366573\n",
      "Epoch #40 end\n",
      "2018-12-16 02:21:33.200869\n",
      "epoch: 41\n",
      "Epoch #41 start\n",
      "2018-12-16 02:21:33.200869\n",
      "Epoch #41 end\n",
      "2018-12-16 02:25:38.838456\n",
      "epoch: 42\n",
      "Epoch #42 start\n",
      "2018-12-16 02:25:38.838456\n",
      "Epoch #42 end\n",
      "2018-12-16 02:29:05.138775\n",
      "epoch: 43\n",
      "Epoch #43 start\n",
      "2018-12-16 02:29:05.138775\n",
      "Epoch #43 end\n",
      "2018-12-16 02:32:40.923402\n",
      "epoch: 44\n",
      "Epoch #44 start\n",
      "2018-12-16 02:32:40.923402\n",
      "Epoch #44 end\n",
      "2018-12-16 02:36:46.016171\n",
      "epoch: 45\n",
      "Epoch #45 start\n",
      "2018-12-16 02:36:46.017137\n",
      "Epoch #45 end\n",
      "2018-12-16 02:41:15.129748\n",
      "epoch: 46\n",
      "Epoch #46 start\n",
      "2018-12-16 02:41:15.130746\n",
      "Epoch #46 end\n",
      "2018-12-16 02:44:52.369944\n",
      "epoch: 47\n",
      "Epoch #47 start\n",
      "2018-12-16 02:44:52.369944\n",
      "Epoch #47 end\n",
      "2018-12-16 02:49:05.062794\n",
      "epoch: 48\n",
      "Epoch #48 start\n",
      "2018-12-16 02:49:05.062794\n",
      "Epoch #48 end\n",
      "2018-12-16 02:54:03.740774\n",
      "epoch: 49\n",
      "Epoch #49 start\n",
      "2018-12-16 02:54:03.740774\n",
      "Epoch #49 end\n",
      "2018-12-16 02:59:47.411564\n",
      "epoch: 50\n",
      "Epoch #50 start\n",
      "2018-12-16 02:59:47.411564\n",
      "Epoch #50 end\n",
      "2018-12-16 03:05:25.234972\n",
      "epoch: 51\n",
      "Epoch #51 start\n",
      "2018-12-16 03:05:25.234972\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d7d245bfd205>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"epoch:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m0.00002\u001b[0m  \u001b[1;31m# decrease the learning rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_alpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, documents, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[0;32m    796\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[0;32m    554\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[0;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m             \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocks if workers too slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# a thread reporting that it finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model.train(tagged_data, total_examples=model.corpus_count, epochs=500)\n",
    "# model.save(\"d2v.model\")\n",
    "# print(\"Model Saved\")\n",
    "# original trained for ~125 epochs over ~9 hours\n",
    "for epoch in range(500):\n",
    "    print(\"epoch:\",epoch)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=1)\n",
    "    model.alpha -= 0.00002  # decrease the learning rate\n",
    "    model.min_alpha = model.alpha\n",
    "model.save(\"small_d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>gilded</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.194211e+06</td>\n",
       "      <td>3.194211e+06</td>\n",
       "      <td>3.194211e+06</td>\n",
       "      <td>3.194211e+06</td>\n",
       "      <td>3.194211e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.043509e-02</td>\n",
       "      <td>1.517490e+09</td>\n",
       "      <td>1.984841e-04</td>\n",
       "      <td>1.518807e+09</td>\n",
       "      <td>8.892290e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.414832e-01</td>\n",
       "      <td>2.646685e+04</td>\n",
       "      <td>1.729891e-02</td>\n",
       "      <td>7.827520e+04</td>\n",
       "      <td>1.245221e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.517443e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.518478e+09</td>\n",
       "      <td>-8.660000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.517464e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.518816e+09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.517497e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.518825e+09</td>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.517513e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.518833e+09</td>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.517530e+09</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>1.518842e+09</td>\n",
       "      <td>4.832700e+04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       controversiality   created_utc        gilded  retrieved_on  \\\n",
       "count      3.194211e+06  3.194211e+06  3.194211e+06  3.194211e+06   \n",
       "mean       2.043509e-02  1.517490e+09  1.984841e-04  1.518807e+09   \n",
       "std        1.414832e-01  2.646685e+04  1.729891e-02  7.827520e+04   \n",
       "min        0.000000e+00  1.517443e+09  0.000000e+00  1.518478e+09   \n",
       "25%        0.000000e+00  1.517464e+09  0.000000e+00  1.518816e+09   \n",
       "50%        0.000000e+00  1.517497e+09  0.000000e+00  1.518825e+09   \n",
       "75%        0.000000e+00  1.517513e+09  0.000000e+00  1.518833e+09   \n",
       "max        1.000000e+00  1.517530e+09  1.200000e+01  1.518842e+09   \n",
       "\n",
       "              score  \n",
       "count  3.194211e+06  \n",
       "mean   8.892290e+00  \n",
       "std    1.245221e+02  \n",
       "min   -8.660000e+02  \n",
       "25%    1.000000e+00  \n",
       "50%    2.000000e+00  \n",
       "75%    4.000000e+00  \n",
       "max    4.832700e+04  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "author_cakeday,id, link_id,retrieved_on,subreddit_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frame=frame.drop(columns=['author_cakeday','id','link_id','retrieved_on','subreddit_id'])\n",
    "frame=frame.drop(columns='parent_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.to_csv(\"clean_reddit_02_01_18.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (4,5,6,9,10,12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>can_gild</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>edited</th>\n",
       "      <th>gilded</th>\n",
       "      <th>is_submitter</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouthfulPhotographer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Welcome to generation void</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1.51744e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/tumblr/comments/7uaobc/nihilism_across_gene...</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>tumblr</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jasonklacour</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Welcome</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1.51744e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/pics/comments/7ude45/7_years_later_im_offic...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>pics</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Assassin2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm 16 and the friend told me he was joking af...</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1.51744e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>/r/legaladvice/comments/7uegdc/took_a_awful_jo...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>legaladvice</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kawaiicicle</td>\n",
       "      <td>Employee</td>\n",
       "      <td>Assistant Manager</td>\n",
       "      <td>What? \\nIt’s a niche rpg. Most rpg fans are ad...</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1.51744e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/GameStop/comments/7u7mps/is_this_possible/d...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>GameStop</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recklessmaterialism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>solid!</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1.51744e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>/r/Seattle/comments/7udrqf/in_town_for_only_a_...</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author author_flair_css_class  author_flair_text  \\\n",
       "0  YouthfulPhotographer                    NaN                NaN   \n",
       "1          jasonklacour                    NaN                NaN   \n",
       "2          Assassin2000                    NaN                NaN   \n",
       "3           kawaiicicle               Employee  Assistant Manager   \n",
       "4   recklessmaterialism                    NaN                NaN   \n",
       "\n",
       "                                                body can_gild  \\\n",
       "0                        Welcome to generation void      True   \n",
       "1                                            Welcome     True   \n",
       "2  I'm 16 and the friend told me he was joking af...     True   \n",
       "3  What? \\nIt’s a niche rpg. Most rpg fans are ad...     True   \n",
       "4                                             solid!     True   \n",
       "\n",
       "  controversiality  created_utc distinguished edited gilded is_submitter  \\\n",
       "0                0  1.51744e+09           NaN  False      0        False   \n",
       "1                0  1.51744e+09           NaN  False      0        False   \n",
       "2                0  1.51744e+09           NaN  False      0         True   \n",
       "3                0  1.51744e+09           NaN  False      0        False   \n",
       "4                0  1.51744e+09           NaN  False      0         True   \n",
       "\n",
       "                                           permalink score stickied  \\\n",
       "0  /r/tumblr/comments/7uaobc/nihilism_across_gene...     7    False   \n",
       "1  /r/pics/comments/7ude45/7_years_later_im_offic...     1    False   \n",
       "2  /r/legaladvice/comments/7uegdc/took_a_awful_jo...     1    False   \n",
       "3  /r/GameStop/comments/7u7mps/is_this_possible/d...     0    False   \n",
       "4  /r/Seattle/comments/7udrqf/in_town_for_only_a_...    -1    False   \n",
       "\n",
       "     subreddit subreddit_type  \n",
       "0       tumblr         public  \n",
       "1         pics         public  \n",
       "2  legaladvice         public  \n",
       "3     GameStop         public  \n",
       "4      Seattle         public  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"clean_reddit_02_01_18.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating flesch_kincaid_readability\n",
    "#vocabulary difficulty\n",
    "def generatefkReadability(frame):\n",
    "    comments=frame.body\n",
    "    textrating=[]\n",
    "    for comment in comments:\n",
    "        textrating.append(textstat.flesch_kincaid_grade(comment))\n",
    "    #textstat.flesch_reading_ease(test_data)\n",
    "    #textstat.smog_index(test_data)\n",
    "    #textstat.flesch_kincaid_grade(test_data)\n",
    "    #textstat.coleman_liau_index(test_data)\n",
    "    #textstat.automated_readability_index(test_data)\n",
    "    #textstat.dale_chall_readability_score(test_data)\n",
    "    #textstat.difficult_words(test_data)\n",
    "    #textstat.linsear_write_formula(test_data)\n",
    "    #textstat.gunning_fog(test_data)\n",
    "    #textstat.text_standard(test_data)\n",
    "\n",
    "    return textrating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for length of comment\n",
    "def wordLengthGenerator(frame):\n",
    "    comments=frame.body\n",
    "    lengths=[]\n",
    "    for comment in comments:\n",
    "        lengths.append(len(comment))\n",
    "    return lengths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the ratio of uppercase to lower case letters\n",
    "#returns 0 if length is zero\n",
    "def upperCaseGenerator(frame):\n",
    "    comments=frame.body\n",
    "    ratios=[]\n",
    "    for comment in comments:\n",
    "        count = 0\n",
    "        for c in comment:\n",
    "            if(c.isupper()):\n",
    "                count = count + 1\n",
    "        if(len(comment)>0):\n",
    "            ratioval=float(count/len(comment))\n",
    "        else:\n",
    "            ratioval=0\n",
    "        ratios.append(ratioval)\n",
    "    return ratios\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts number of questions in a comment\n",
    "#by counting number of question marks\n",
    "def numberOfQuestionsInAComment(frame):\n",
    "    comments=frame.body\n",
    "    questions=[]\n",
    "    for comment in comments:\n",
    "        count=0\n",
    "        for c in comment:\n",
    "            if(c=='?'):\n",
    "                count=count+1\n",
    "        questions.append(count)\n",
    "    return questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive method of determining whether or\n",
    "#not the title is a question\n",
    "def isTitleAQuestion(frame):\n",
    "    START_WORDS = [\"who\", \"what\", \"when\", \"where\", \"why\", \"how\", \"is\", \"can\", \"does\", \"do\",\n",
    "                  \"could\",\"should\",\"would\",\"which\",\"whose\",\"whom\",\"are\",]\n",
    "    permalinks=frame.permalink\n",
    "    isQuestion=[]\n",
    "    for permalink in permalinks:\n",
    "        splitty=permalink.split('/')\n",
    "        title=splitty[5]\n",
    "        wordsInTitle=title.split('_')\n",
    "        firstWord=wordsInTitle[0]\n",
    "        if(firstWord in START_WORDS):\n",
    "            isQuestion.append(1)\n",
    "        else:\n",
    "            isQuestion.append(0)\n",
    "    \n",
    "    return isQuestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subreddit name separation and extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(\"final_d2v_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Risha\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2160395', 0.773759126663208), ('1433720', 0.7351023554801941), ('2640563', 0.7267253994941711), ('769285', 0.7115885019302368), ('1871186', 0.7106360197067261), ('826092', 0.6876705288887024), ('1854229', 0.6779251098632812), ('381699', 0.6749313473701477), ('1423085', 0.6657606363296509), ('1966050', 0.6602393984794617)]\n"
     ]
    }
   ],
   "source": [
    "print(model.docvecs.most_similar(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = \"king queen man\".split()\n",
    "# new_vec = model.infer_vector(words)\n",
    "def similar(i, m):\n",
    "    print(\"Original: \", frame['body'][i].replace(\"\\r\", \" \"))\n",
    "    similar_i = m.docvecs.most_similar(i)\n",
    "    for doc_i, sim in similar_i:\n",
    "        print(\"Similarity: \", str(sim), \"\\nDoc \", str(doc_i), \":\\n\", frame['body'][int(doc_i)].replace(\"\\r\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What? \r",
      " It’s a niche rpg. Most rpg fans are adults. \r",
      " Why’d ya have to bring race or weight into it? Yeesh kid. \n"
     ]
    }
   ],
   "source": [
    "print(frame['body'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "carriage = [(i, body) for i, body in enumerate(frame['body']) if '\\r' in body]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfgdfg\r",
      "sdffdsdfgsdfg\n"
     ]
    }
   ],
   "source": [
    "print(\"dfgdfg\\rsdffdsdfgsdfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3,\n",
       "  'What? \\r It’s a niche rpg. Most rpg fans are adults. \\r Why’d ya have to bring race or weight into it? Yeesh kid. '),\n",
       " (8,\n",
       "  \"I'm sorry that happened to you, I had a similar thing happen to me and it worked out okay. I went to a first aid class with the rest of my uni cohort and fainted after doing CPR (I was doing a fitness challenge at the time and probably hadn't eaten/drunken enough that day, so my own fault really). I was out for maybe 10 seconds at most, and when I came to my classmates were all super concerned. One guy gave me his sandwich, another went to buy me some Powerade and bananas. I went home, then the next time I went to class a couple of friends asked me if I was okay and that was it. Never brought up again, I wasn't the focus of attention at all. It was just a couple of friends checking up on me, as if I'd just had the flu or something. \\r \\r And I also kinda dealt with it through humour, which is kinda my go-to. For example, I had to do a speech later that year to the same class and I made a joke about how I never could've been a doctor because I couldn't even get through a first aid class without fainting. I still use that all the time when people ask me why I chose my career (biomed engineering). Being able to laugh with people about something frightening or bad can really help you to realise it's no big deal, but I also get it if that wouldn't work for you - it's just an idea!\"),\n",
       " (9,\n",
       "  \"You do realize that January is practically over, right? \\r \\r It's already February in China and in much of Europe, are you seriously expecting some sort of massive plummet in the next 8 hours? \\r \"),\n",
       " (19,\n",
       "  'Personal experience: my first job was in a startup. \\r The Pay was not great but I used that opportunity to grow not only on a technical level but also on a biz level. \\r This helped me a lot on my second job. \\r On my second job, if issues arised, I would recommend solutions and lead implementation based on the experience i has from startup. I was able to participate in discussions with biz teams and provide explanation why the solution was good etc.\\r My  confidence was based on my startup experience as well as tech knowledge ( but as a young woman in tech, confidence based on past experience was important to me)\\r On my second job, I was promoted within 6 months to a program manager role and leading major new tech projects.\\r \\r I think it’s all worth it, startups are like an apprenticeship to me.\\r '),\n",
       " (22, 'Ampidextrious \\r \\r Super Mario odyssey would be cool'),\n",
       " (33,\n",
       "  \"Is there one that doesn't have any words in it?\\r \\r I feel I'd earn the most if I never had to talk to any other fucker ever!\"),\n",
       " (35,\n",
       "  \"The outfit of the day today features a pale pink houmongi (semi-formal kimono) with a *kumihimo* motif. Kumihimo are traditional braided cords, ranging from simple solid-colours to complex patterns and motifs. Traditionally, they were used for anything that needed binding, like samurai armour or to tie gifts closed/together. Nowadays, they're mostly used for kimono, such as *obijime* or the cords used to fasten a *haori* closed. Interestingly enough, this person has paired the kimono with a similarly-themed obi.\\r \\r The tradition of braiding *kumihimo* has made a bit of a comeback thanks to the wildly popular and critically acclaimed anime film, *Your Name* (*Kimi no Na wa*) by Makoto Shinkai.\"),\n",
       " (40,\n",
       "  'I disagree. All those elements have been around TH for years. There\\'s occasionally someone who causes way more drama, but that passes when they leave eventually. Yuudai isn\\'t even as bad as some of the other people who have been on the show. He\\'s not malicious, he\\'s just dumb, young &amp; selfish.\\r \\r As for the supermarket \"incident\", it\\'s possible that nothing appeared to happen to the crew. The crew aren\\'t always around, and aren\\'t always filming 24/7 so there\\'s going to be stuff they miss. I think Yuudai\\'s mood likely turned as they were coming home or when they got home, because he probably felt like Ami was rude, or inconsiderate by agreeing to the date only to change her mind on the day. Yuudai seemed surprised to hear Ami say it was good until he got home, seemed like the impression she gave was that it wasn\\'t a lot of fun at all.'),\n",
       " (46,\n",
       "  'If you would like to claim this post, please respond to this comment with the word `claiming` or `claim` in your response. I will automatically update the flair so that only one person is working on a post at any given time.\\r \\r When you\\'re done, please comment again with `done`. Your flair will be updated to reflect the number of posts you\\'ve transcribed and the will be marked as completed.\\r \\r This is a(n) image post, so please use the following formatting:\\r \\r ---\\r \\r **Note:** To use these format guides, all you have to do is copy and paste everything after the line break and before the line break preceding \"*END*\", and replace the carets and the values inside them. The two asterisks turn any text between them into boldtext, and the three hyphens in a row when separated by a blank row above and below will become a horizontal rule. \\r\\r \\r\\r **Remember:** We want to transcribe the text directly, please do not make corrections to typos or grammatical errors, but feel free to use [sic] to indicate that the text is exactly as in the original. Please copy and paste any emojis into your transcription.\\r\\r \\r\\r ---\\r\\r \\r\\r [**Art and Images without Text**](https://www.reddit.com/r/TranscribersOfReddit/wiki/format/images/no_text)\\r\\r \\r\\r [**Images with Text**](https://www.reddit.com/r/TranscribersOfReddit/wiki/format/images/text)\\r\\r \\r\\r [**4chan &amp; Pictures of Greentext**](https://www.reddit.com/r/TranscribersOfReddit/wiki/format/images/greentext)\\r\\r \\r\\r [**Reddit Posts &amp; Comments**](https://www.reddit.com/r/TranscribersOfReddit/wiki/format/images/reddit)\\r\\r \\r\\r [**Facebook Posts and Comments**](https://www.reddit.com/r/TranscribersOfReddit/wiki/format/images/facebook)\\r\\r \\r\\r [**Twitter Posts and Replies**](https://www.reddit.com/r/TranscribersOfReddit/wiki/format/images/twitter)\\r\\r \\r\\r [**Comics**](https://www.reddit.com/r/TranscribersOfReddit/wiki/format/images/comics)\\r\\r \\r\\r [**Gifs**](https://www.reddit.com/r/TranscribersOfReddit/wiki/format/images/gifs)\\r\\r \\r\\r [**Text Messages (SMS)**](https://www.reddit.com/r/TranscribersOfReddit/wiki/format/images/textmessages)\\r\\r \\r\\r [**Code**](https://www.reddit.com/r/TranscribersOfReddit/wiki/format/images/code)\\r\\r \\r\\r [**Meme**](https://www.reddit.com/r/TranscribersOfReddit/wiki/format/images/meme)\\r\\r \\r\\r [**Other Sources**](https://www.reddit.com/r/TranscribersOfReddit/wiki/format/images/other)\\r\\r \\r\\r ---\\r\\r \\r\\r [You can see some examples here of what we\\'re looking for.](https://www.reddit.com/r/TranscribersOfReddit/wiki/examples/images)\\r \\r ---\\r \\r Footer\\r ---\\r \\r When you\\'re done, please put the following footer at the **bottom** of your post:\\r \\r ---\\r \\r     ^^I\\'m&amp;#32;a&amp;#32;human&amp;#32;volunteer&amp;#32;content&amp;#32;transcriber&amp;#32;for&amp;#32;Reddit&amp;#32;and&amp;#32;you&amp;#32;could&amp;#32;be&amp;#32;too!&amp;#32;[If&amp;#32;you\\'d&amp;#32;like&amp;#32;more&amp;#32;information&amp;#32;on&amp;#32;what&amp;#32;we&amp;#32;do&amp;#32;and&amp;#32;why&amp;#32;we&amp;#32;do&amp;#32;it,&amp;#32;click&amp;#32;here!](https://www.reddit.com/r/TranscribersOfReddit/wiki/index)\\r \\r ---\\r \\r If you have any questions, feel free to [message the mods!](https://www.reddit.com/message/compose?to=%2Fr%2FTranscribersOfReddit&amp;subject=General%20Question&amp;message=)\\r \\r ---\\r \\r v0.4.2 | This message was posted by a bot. | [FAQ](https://www.reddit.com/r/TranscribersOfReddit/wiki/index) | [Source](https://github.com/GrafeasGroup/tor) | Questions? [Message the mods!](https://www.reddit.com/message/compose?to=%2Fr%2FTranscribersOfReddit&amp;subject=Bot%20Question&amp;message=)'),\n",
       " (48,\n",
       "  'Nope, it was a hairy mofo with a 12-incher that did the dirty with them there fish and you know it, mate!!\\r \\r Stop actin like a freakin weasel, kid, and own your perversion, geez.')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carriage[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = []\n",
    "for i, _d in carriage[:10]:\n",
    "    tagged_data.append(TaggedDocument(words=word_tokenize(_d.lower()),tags=[str(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['what', '?', 'it', '’', 's', 'a', 'niche', 'rpg', '.', 'most', 'rpg', 'fans', 'are', 'adults', '.', 'why', '’', 'd', 'ya', 'have', 'to', 'bring', 'race', 'or', 'weight', 'into', 'it', '?', 'yeesh', 'kid', '.'], tags=['3'])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2160395', 0.773759126663208), ('1433720', 0.7351023554801941), ('2640563', 0.7267253994941711), ('769285', 0.7115885019302368), ('1871186', 0.7106360197067261), ('826092', 0.6876705288887024), ('1854229', 0.6779251098632812), ('381699', 0.6749313473701477), ('1423085', 0.6657606363296509), ('1966050', 0.6602393984794617)]\n"
     ]
    }
   ],
   "source": [
    "print(similar_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Man I'm jelly you got 2, but after playing with a 55% I realise you don't even need a dupe on him the team is so op. I tried not to even super attack to get the transformation and even then it's pretty hard and only possible on last stage.\n",
      "Similarity:  0.7767763137817383 \n",
      "Doc  551337 :\n",
      " lol no problem. do u need the LR as lead or Teq VB? I got both\n",
      "Similarity:  0.7724827527999878 \n",
      "Doc  1802996 :\n",
      " Because it's SUPER good in this patch. In 90% of my games, all I get to do is farm. And in these types of games you just farm for the first 30min as jugg for example and get bfury-&gt;manta-&gt;blink and just steamroll over the opponents.    Currently the meta is either all in fight (vs lycan lineups etc) and then jugg sucks. Or its a tradeoff between cores with farm.\n",
      "Similarity:  0.7687543630599976 \n",
      "Doc  572361 :\n",
      " Yeah but I've only got that on Zelgius and then the one guy who's got it as a seal. I run into at least 4 Ninos an Arena Assault run anymore.    I guess I can make use of that panic item from AA. :thonking:\n",
      "Similarity:  0.7667709589004517 \n",
      "Doc  1733625 :\n",
      " Did you ever fought against team with 10* ormus? Ormus is pretty weak pvp hero the only use for him in pvp is if you need another fortress for aura and you got only ormus... Most of the time he won't even heal and get cced before even doing anything. \n",
      "Similarity:  0.7658149600028992 \n",
      "Doc  2794648 :\n",
      " You did the super twice? Honestly the best reason for this was when he tried to get a gotenks assist. You literally killed two birds with one stone, if he would’ve just waited for you he would’ve Atleast had gotenks to fight, I really wanna see Golden freiza and level 3 spark and watch the shit fly then! \n",
      "Similarity:  0.7638774514198303 \n",
      "Doc  1890732 :\n",
      " Idk but before leia became popular on the map I was at least able to try sniping which I enjoy doing, but as the FO I can’t even do that because I get insta killed, she kills tie fighters in like 2 shots as well and I don’t think that should be a thing especially since it locks on\n",
      "Similarity:  0.7591861486434937 \n",
      "Doc  2395929 :\n",
      " Yeah I'm thinking it has to have been either because if it was super OP they wouldn't had left it in the game. But even after I gave up hope after my 2nd char died he still kept the combo going. I'm about to hit up practice and see if I can replicate it\n",
      "Similarity:  0.7582089900970459 \n",
      "Doc  1739570 :\n",
      " I might not have played enough games to judge but out of the 9 or 10 games I played where I joined an SOS the leader kept disbanding the SOS right before the monster died so only he got the kill and we get a failed game. Either that or someone just attacks you at the end so you can't carve the monster. I've probably just been unlucky \n",
      "Similarity:  0.7581894397735596 \n",
      "Doc  2387848 :\n",
      " Being outnumbered as a HL is pretty much just parry your heart out and throw in a target switched double Celtic every time you feel like you won't instantly die doing so.\n",
      "Similarity:  0.756526529788971 \n",
      "Doc  664762 :\n",
      " so it's basically 3v3 as it's always with valk too\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Risha\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "similar(434, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was '"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame['body'][421663].replace('\\r', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_to(doc, m, df):\n",
    "    words = word_tokenize(doc.lower())\n",
    "    vec = m.infer_vector(words)\n",
    "    print(\"Original: \", doc.replace(\"\\r\", \" \"))\n",
    "    similar_i = m.docvecs.most_similar([vec])\n",
    "    for doc_i, sim in similar_i:\n",
    "        print(\"Similarity: \", str(sim), \"\\nDoc \", str(doc_i), \":\\n\", df['body'][int(doc_i)].replace(\"\\r\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  avatar sucks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Risha\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity:  0.5717811584472656 \n",
      "Doc  422507 :\n",
      " james johnson sucks\n",
      "Similarity:  0.5379082560539246 \n",
      "Doc  464461 :\n",
      " Sucks to be you[.](https://imgur.com/cXA7XxW)\n",
      "Similarity:  0.5319690704345703 \n",
      "Doc  2411277 :\n",
      " Hollywood sucks sucks sucks \n",
      "Similarity:  0.5304902195930481 \n",
      "Doc  1915666 :\n",
      " This guy sucks ^\n",
      "Similarity:  0.5294104218482971 \n",
      "Doc  1774504 :\n",
      " Sucks to suck.\n",
      "Similarity:  0.5059899091720581 \n",
      "Doc  318465 :\n",
      " One that sucks sucks . \n",
      "Similarity:  0.504196286201477 \n",
      "Doc  2108386 :\n",
      " This Black Mirror episode sucks\n",
      "Similarity:  0.5040415525436401 \n",
      "Doc  2873830 :\n",
      " Being childless and 60+ years old sucks. \n",
      "Similarity:  0.4917881190776825 \n",
      "Doc  2580845 :\n",
      " Josh Allen sucks. \n",
      "Similarity:  0.49134188890457153 \n",
      "Doc  2303977 :\n",
      " Buying and selling houses sucks. A lot.\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(\"avatar sucks\".lower())\n",
    "vec = model.infer_vector(words, epochs=500)\n",
    "#print(vec)\n",
    "print(\"Original: \", \"avatar sucks\".replace(\"\\r\", \" \"))\n",
    "similar_i = model.docvecs.most_similar([vec])\n",
    "for doc_i, sim in similar_i:\n",
    "    print(\"Similarity: \", str(sim), \"\\nDoc \", str(doc_i), \":\\n\", frame['body'][int(doc_i)].replace(\"\\r\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  airbender\n",
      "Similarity:  0.7820019125938416 \n",
      "Doc  604163 :\n",
      " .\n",
      "Similarity:  0.7577211260795593 \n",
      "Doc  221724 :\n",
      " I\n",
      "Similarity:  0.7492688298225403 \n",
      "Doc  1853457 :\n",
      " Glaub eine Frau\n",
      "Similarity:  0.7422033548355103 \n",
      "Doc  1214998 :\n",
      " What a *darn* shame..  ***  ^^Darn ^^Counter: ^^56396\n",
      "Similarity:  0.7377957105636597 \n",
      "Doc  1931480 :\n",
      " 150 Partnership \n",
      "Similarity:  0.7355886697769165 \n",
      "Doc  1408156 :\n",
      " Ist ein Fehler. \n",
      "Similarity:  0.7283194065093994 \n",
      "Doc  1049938 :\n",
      " .\n",
      "Similarity:  0.7213022112846375 \n",
      "Doc  353249 :\n",
      " .\n",
      "Similarity:  0.7200831174850464 \n",
      "Doc  2135047 :\n",
      " Ich liebe dich, Özil \n",
      "Similarity:  0.7181810140609741 \n",
      "Doc  2220896 :\n",
      " I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Risha\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "similar_to(\"airbender\", model, frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
