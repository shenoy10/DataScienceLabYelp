{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Risha\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n",
      "C:\\Users\\Risha\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "import textdistance as td\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Risha\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (4,5,6,9,10,12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>can_gild</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>edited</th>\n",
       "      <th>gilded</th>\n",
       "      <th>is_submitter</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouthfulPhotographer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Welcome to generation void</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1.51744e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/tumblr/comments/7uaobc/nihilism_across_gene...</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>tumblr</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jasonklacour</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Welcome</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1.51744e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/pics/comments/7ude45/7_years_later_im_offic...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>pics</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Assassin2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm 16 and the friend told me he was joking af...</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1.51744e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>/r/legaladvice/comments/7uegdc/took_a_awful_jo...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>legaladvice</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kawaiicicle</td>\n",
       "      <td>Employee</td>\n",
       "      <td>Assistant Manager</td>\n",
       "      <td>What? \\nIt’s a niche rpg. Most rpg fans are ad...</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1.51744e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/GameStop/comments/7u7mps/is_this_possible/d...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>GameStop</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recklessmaterialism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>solid!</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1.51744e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>/r/Seattle/comments/7udrqf/in_town_for_only_a_...</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author author_flair_css_class  author_flair_text  \\\n",
       "0  YouthfulPhotographer                    NaN                NaN   \n",
       "1          jasonklacour                    NaN                NaN   \n",
       "2          Assassin2000                    NaN                NaN   \n",
       "3           kawaiicicle               Employee  Assistant Manager   \n",
       "4   recklessmaterialism                    NaN                NaN   \n",
       "\n",
       "                                                body can_gild  \\\n",
       "0                        Welcome to generation void      True   \n",
       "1                                            Welcome     True   \n",
       "2  I'm 16 and the friend told me he was joking af...     True   \n",
       "3  What? \\nIt’s a niche rpg. Most rpg fans are ad...     True   \n",
       "4                                             solid!     True   \n",
       "\n",
       "  controversiality  created_utc distinguished edited gilded is_submitter  \\\n",
       "0                0  1.51744e+09           NaN  False      0        False   \n",
       "1                0  1.51744e+09           NaN  False      0        False   \n",
       "2                0  1.51744e+09           NaN  False      0         True   \n",
       "3                0  1.51744e+09           NaN  False      0        False   \n",
       "4                0  1.51744e+09           NaN  False      0         True   \n",
       "\n",
       "                                           permalink score stickied  \\\n",
       "0  /r/tumblr/comments/7uaobc/nihilism_across_gene...     7    False   \n",
       "1  /r/pics/comments/7ude45/7_years_later_im_offic...     1    False   \n",
       "2  /r/legaladvice/comments/7uegdc/took_a_awful_jo...     1    False   \n",
       "3  /r/GameStop/comments/7u7mps/is_this_possible/d...     0    False   \n",
       "4  /r/Seattle/comments/7udrqf/in_town_for_only_a_...    -1    False   \n",
       "\n",
       "     subreddit subreddit_type  \n",
       "0       tumblr         public  \n",
       "1         pics         public  \n",
       "2  legaladvice         public  \n",
       "3     GameStop         public  \n",
       "4      Seattle         public  "
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv = \"./Data/clean_reddit_02_01_18.csv\"\n",
    "train = pd.read_csv(train_csv, encoding=\"utf-8\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove corrupt rows and store as different .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(num):\n",
    "    if pd.isna(num):\n",
    "        return True\n",
    "    try:\n",
    "        float(num)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_integer(num):\n",
    "    if pd.isna(num):\n",
    "        return True\n",
    "    try:\n",
    "        int(num)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def valid_name(name):\n",
    "    name_regex = re.compile(r\"\\A[A-Za-z0-9][A-Za-z0-9_-]{1,20}\\Z\")\n",
    "    return bool(name_regex.match(name)) or pd.isna(name)\n",
    "\n",
    "def valid_body(body):\n",
    "    return len(body.strip()) > 0 or pd.isna(body)\n",
    "\n",
    "def is_boolean(boo):\n",
    "    return (str(boo) in ['True', 'False']) or pd.isna(boo)\n",
    "\n",
    "def valid_controversiality(controversiality):\n",
    "    return (is_number(controversiality) and float(controversiality) <= 1 and float(controversiality) >= 0) or pd.isna(controversiality)\n",
    "\n",
    "def valid_utc(utc):\n",
    "    return is_number(utc) and (len(str(utc)) == 12 or len(str(utc)) == 10) or pd.isna(utc)\n",
    "\n",
    "def valid_distinguished(distinguished):\n",
    "    return (str(distinguished) in ['nan', 'moderator', 'admin', 'special']) or pd.isna(distinguished)\n",
    "\n",
    "def valid_permalink(permalink):\n",
    "    title_regex = re.compile(r\"(comments)/[^/]*/(?P<title>[^/]*)\")\n",
    "    return title_regex.search(permalink).group('title') != None or pd.isna(permalink)\n",
    "\n",
    "def valid_subreddit_type(subreddit_type):\n",
    "    return str(subreddit_type) in ['public', 'restricted', 'user'] or pd.isna(subreddit_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_corrupt_rows(df):\n",
    "    corrupt_rows = []\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        is_invalid_feature_info = {'author': False, 'author_flair_css_class': False, 'author_flair_text': False, 'body': False,\n",
    "       'can_gild': False, 'controversiality': False, 'created_utc': False, 'distinguished': False,\n",
    "       'edited': False, 'gilded': False, 'is_submitter': False, 'permalink': False, 'score': False,\n",
    "       'stickied': False, 'subreddit': False, 'subreddit_type': False}\n",
    "        try:\n",
    "            if not valid_name(row['author']):\n",
    "                is_invalid_feature_info['author'] = True\n",
    "            if not valid_body(row['body']):\n",
    "                is_invalid_feature_info['body'] = True\n",
    "            if not is_boolean(row['can_gild']) :\n",
    "                is_invalid_feature_info['can_gild'] = True\n",
    "            if not valid_controversiality(row['controversiality']):\n",
    "                is_invalid_feature_info['controversiality'] = True\n",
    "            if not valid_utc(row['created_utc']):\n",
    "                is_invalid_feature_info['created_utc'] = True\n",
    "            if not valid_distinguished(row['distinguished']):\n",
    "                is_invalid_feature_info['distinguished'] = True\n",
    "            if not is_boolean(row['edited']):\n",
    "                is_invalid_feature_info['edited'] = True\n",
    "            if not is_integer(row['gilded']):\n",
    "                is_invalid_feature_info['gilded'] = True\n",
    "            if not is_boolean(row['is_submitter']): \n",
    "                is_invalid_feature_info['is_submitter'] = True\n",
    "            if not valid_permalink(row['permalink']): \n",
    "                is_invalid_feature_info['permalink'] = True\n",
    "            if not is_integer(row['score']): \n",
    "                is_invalid_feature_info['score'] = True\n",
    "            if not is_boolean(row['stickied']): \n",
    "                is_invalid_feature_info['stickied'] = True\n",
    "            if not valid_name(row['subreddit']): \n",
    "                is_invalid_feature_info['subreddit'] = True\n",
    "            if not valid_subreddit_type(row['subreddit_type']):\n",
    "                is_invalid_feature_info['subreddit_type'] = True\n",
    "\n",
    "            if any(list(is_invalid_feature_info.values())):\n",
    "                corrupt_row_info = (index, is_invalid_feature_info)\n",
    "                corrupt_rows.append(corrupt_row_info)\n",
    "        except:\n",
    "            corrupt_row_info = (index, is_invalid_feature_info)\n",
    "            corrupt_rows.append(corrupt_row_info)\n",
    "    return corrupt_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_and_corrupt_df(df):\n",
    "    corrupt_row_indices = find_corrupt_rows(df)\n",
    "    print(\"Number of corrupt rows: \", len(corrupt_row_indices), \"Number of valid rows: \", str(len(df)-len(corrupt_row_indices)))\n",
    "    indices_to_drop, errors = zip(*corrupt_row_indices)\n",
    "    indices_to_drop = [int(i) for i in indices_to_drop]\n",
    "    corrupt_rows = df.iloc[indices_to_drop,:]\n",
    "    valid_rows = df.copy()\n",
    "    valid_rows = valid_rows.drop(df.index[indices_to_drop])\n",
    "    return (valid_rows, corrupt_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of corrupt rows:  308247 Number of valid rows:  2885987\n"
     ]
    }
   ],
   "source": [
    "valid_train, invalid_train = get_valid_and_corrupt_df(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_train.to_csv(\"reddit_train.csv\", encoding='utf-8', index=False)\n",
    "invalid_train.to_csv(\"invalid_reddit_train.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data further\n",
    "\n",
    "Remove newlines from comment bodies, assert data types for features, replace na/null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_data_types(df):\n",
    "    original_data_types = {'author': str, 'author_flair_css_class': str, 'author_flair_text': str, 'body': str, 'can_gild': bool,\n",
    "              'controversiality': float, 'created_utc': int, 'distinguished': str, 'edited': bool, 'gilded': int,\n",
    "              'is_submitter': bool, 'permalink': str, 'score': int, 'stickied': bool, 'subreddit': str, 'subreddit_type':  str}\n",
    "    return df.astype(original_data_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_missing_values(df):\n",
    "    avg_created_utc = np.mean([time for time in df['created_utc'] if not pd.isna(time)])\n",
    "    avg_controversiality = np.mean([contro for contro in df['controversiality'] if not pd.isna(contro)])\n",
    "    replacements = {'author':'', 'author_flair_css_class':'', 'author_flair_text':'', 'body':'',\n",
    "                    'can_gild':True, 'controversiality': avg_controversiality, 'created_utc':avg_created_utc,\n",
    "                    'distinguished':False, 'edited':False, 'gilded':False, 'is_submitter':False, 'permalink':'',\n",
    "                    'score':0, 'stickied':False, 'subreddit':'', 'subreddit_type':'public'}\n",
    "    return df.fillna(value=replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_bodies_df(df):\n",
    "    df['body'] = pd.Series([str(body).replace('\\n', ' ') for body in df['body']])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "    df = enforce_data_types(df)\n",
    "    print(\"Data types: \", df.dtypes)\n",
    "    df = fill_in_missing_values(df)\n",
    "    df = clean_bodies_df(df)\n",
    "    print(\"Columns with empty values: \", df.columns[df.isna().any()].tolist())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types:  author                     object\n",
      "author_flair_css_class     object\n",
      "author_flair_text          object\n",
      "body                       object\n",
      "can_gild                     bool\n",
      "controversiality          float64\n",
      "created_utc                 int32\n",
      "distinguished              object\n",
      "edited                       bool\n",
      "gilded                      int32\n",
      "is_submitter                 bool\n",
      "permalink                  object\n",
      "score                       int32\n",
      "stickied                     bool\n",
      "subreddit                  object\n",
      "subreddit_type             object\n",
      "dtype: object\n",
      "Columns with empty values:  []\n"
     ]
    }
   ],
   "source": [
    "clean_valid_train = clean(valid_train.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_valid_train.to_csv(\"reddit_train.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiments(df):\n",
    "    sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "    positive = []\n",
    "    neutral = []\n",
    "    negative = []\n",
    "    compound = []\n",
    "    for text in df['body']:\n",
    "        sentiment = sentiment_analyzer.polarity_scores(text)\n",
    "        positive.append(sentiment['pos'])\n",
    "        neutral.append(sentiment['neu'])\n",
    "        negative.append(sentiment['neg'])\n",
    "        compound.append(sentiment['compound'])\n",
    "        if len(compound) % 500000 == 0:\n",
    "            print(len(compound), \"/\", len(df['body']))\n",
    "    df['positive_sentiment'] = pd.Series(positive)\n",
    "    df['neutral_sentiment'] = pd.Series(neutral)\n",
    "    df['negative_sentiment'] = pd.Series(negative)\n",
    "    df['compound_sentiment'] = pd.Series(compound)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000 / 2885987\n",
      "1000000 / 2885987\n",
      "1500000 / 2885987\n",
      "2000000 / 2885987\n",
      "2500000 / 2885987\n"
     ]
    }
   ],
   "source": [
    "engineered_train = analyze_sentiments(clean_valid_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract post title from permalink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_post_title(permalink):\n",
    "    title_regex = re.compile(r\"(comments)/[^/]*/(?P<title>[^/]*)\")\n",
    "    return title_regex.search(permalink).group('title').replace('_', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_post_titles(df):\n",
    "    df['post_title'] = pd.Series([extract_post_title(permalink) for permalink in df['permalink']])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>can_gild</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>edited</th>\n",
       "      <th>gilded</th>\n",
       "      <th>...</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>positive_sentiment</th>\n",
       "      <th>neutral_sentiment</th>\n",
       "      <th>negative_sentiment</th>\n",
       "      <th>compound_sentiment</th>\n",
       "      <th>post_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouthfulPhotographer</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Welcome to generation void</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1517443200</td>\n",
       "      <td>nan</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>/r/tumblr/comments/7uaobc/nihilism_across_gene...</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>tumblr</td>\n",
       "      <td>public</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>nihilism across generations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jasonklacour</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Welcome</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1517443200</td>\n",
       "      <td>nan</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>/r/pics/comments/7ude45/7_years_later_im_offic...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>pics</td>\n",
       "      <td>public</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>7 years later im officially an us citizen murica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Assassin2000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>I'm 16 and the friend told me he was joking af...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1517443200</td>\n",
       "      <td>nan</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>/r/legaladvice/comments/7uegdc/took_a_awful_jo...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>legaladvice</td>\n",
       "      <td>public</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>took a awful joke seriously and now im in trouble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kawaiicicle</td>\n",
       "      <td>Employee</td>\n",
       "      <td>Assistant Manager</td>\n",
       "      <td>What? \\r It’s a niche rpg. Most rpg fans are a...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1517443200</td>\n",
       "      <td>nan</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>/r/GameStop/comments/7u7mps/is_this_possible/d...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>GameStop</td>\n",
       "      <td>public</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>is this possible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recklessmaterialism</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>solid!</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1517443200</td>\n",
       "      <td>nan</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>/r/Seattle/comments/7udrqf/in_town_for_only_a_...</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>public</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2244</td>\n",
       "      <td>in town for only a day whats the one place i must</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author author_flair_css_class  author_flair_text  \\\n",
       "0  YouthfulPhotographer                    nan                nan   \n",
       "1          jasonklacour                    nan                nan   \n",
       "2          Assassin2000                    nan                nan   \n",
       "3           kawaiicicle               Employee  Assistant Manager   \n",
       "4   recklessmaterialism                    nan                nan   \n",
       "\n",
       "                                                body  can_gild  \\\n",
       "0                        Welcome to generation void       True   \n",
       "1                                            Welcome      True   \n",
       "2  I'm 16 and the friend told me he was joking af...      True   \n",
       "3  What? \\r It’s a niche rpg. Most rpg fans are a...      True   \n",
       "4                                             solid!      True   \n",
       "\n",
       "   controversiality  created_utc distinguished  edited  gilded  \\\n",
       "0               0.0   1517443200           nan   False       0   \n",
       "1               0.0   1517443200           nan   False       0   \n",
       "2               0.0   1517443200           nan   False       0   \n",
       "3               0.0   1517443200           nan   False       0   \n",
       "4               0.0   1517443200           nan   False       0   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "1                        ...                           \n",
       "2                        ...                           \n",
       "3                        ...                           \n",
       "4                        ...                           \n",
       "\n",
       "                                           permalink score  stickied  \\\n",
       "0  /r/tumblr/comments/7uaobc/nihilism_across_gene...     7     False   \n",
       "1  /r/pics/comments/7ude45/7_years_later_im_offic...     1     False   \n",
       "2  /r/legaladvice/comments/7uegdc/took_a_awful_jo...     1     False   \n",
       "3  /r/GameStop/comments/7u7mps/is_this_possible/d...     0     False   \n",
       "4  /r/Seattle/comments/7udrqf/in_town_for_only_a_...    -1     False   \n",
       "\n",
       "     subreddit subreddit_type positive_sentiment  neutral_sentiment  \\\n",
       "0       tumblr         public              0.500              0.500   \n",
       "1         pics         public              1.000              0.000   \n",
       "2  legaladvice         public              0.317              0.683   \n",
       "3     GameStop         public              0.000              1.000   \n",
       "4      Seattle         public              1.000              0.000   \n",
       "\n",
       "   negative_sentiment  compound_sentiment  \\\n",
       "0                 0.0              0.4588   \n",
       "1                 0.0              0.4588   \n",
       "2                 0.0              0.6249   \n",
       "3                 0.0              0.0000   \n",
       "4                 0.0              0.2244   \n",
       "\n",
       "                                          post_title  \n",
       "0                        nihilism across generations  \n",
       "1   7 years later im officially an us citizen murica  \n",
       "2  took a awful joke seriously and now im in trouble  \n",
       "3                                   is this possible  \n",
       "4  in town for only a day whats the one place i must  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engineered_train = extract_post_titles(engineered_train)\n",
    "engineered_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subreddit mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_subreddit_mentions(body):\n",
    "    try:\n",
    "        return len(re.findall(r\"(\\A|[^/A-Za-z0-9])r/[^\\s]+\", body))\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_subreddit_mentions_df(df):\n",
    "    df['num_subreddit_mentions'] = pd.Series([num_subreddit_mentions(body) for body in df['body']])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_train = num_subreddit_mentions_df(engineered_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_user_mentions(body):\n",
    "    return len(re.findall(r\"(\\A|[^/A-Za-z0-9])u/[^\\s]+\", body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_user_mentions_df(df):\n",
    "    df['num_user_mentions'] = pd.Series([num_user_mentions(body) for body in df['body']])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_train = num_user_mentions_df(engineered_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Has flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_train = fill_in_missing_values(engineered_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_flair_df(df):\n",
    "    df['has_flair'] = pd.Series([(len(flair_a) + len(flair_b)) > 0 for flair_a, flair_b in zip(df['author_flair_css_class'].tolist(), df['author_flair_text'].tolist())])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_train = has_flair_df(engineered_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_outside_links(text):\n",
    "    link_regex = re.compile(r\"(?P<url>https?://[^\\s]+)\")\n",
    "    all_matches = link_regex.findall(text)\n",
    "    outside_matches = [match for match in all_matches if \"reddit.com\" not in match and \"redd.it\" not in match]\n",
    "    return len(outside_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_reddit_links(text):    \n",
    "    link_regex = re.compile(r\"(?P<url>https?://[^\\s]+)\")\n",
    "    all_matches = link_regex.findall(text)\n",
    "    reddit_matches = [match for match in all_matches if \"reddit.com\" in match or \"redd.it\" in match]\n",
    "    return len(reddit_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(body):\n",
    "    link_regex = re.compile(r\"(?P<url>https?://[^\\s]+)\")\n",
    "    all_matches = link_regex.findall(body)\n",
    "    return all_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(df):\n",
    "    df['num_outside_links'] = pd.Series([number_of_outside_links(body) for body in df['body']])\n",
    "    df['num_reddit_links'] = pd.Series([number_of_reddit_links(body) for body in df['body']])\n",
    "#     df['links'] = pd.Series([get_links(body) for body in df['body']])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_train = extract_links(engineered_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "#generating flesch_kincaid_readability\n",
    "#vocabulary difficulty\n",
    "def generatefkReadability(frame):\n",
    "    comments=frame.body\n",
    "    textrating=[]\n",
    "    for comment in comments:\n",
    "    textrating.append(textstat.text_standard(comment),float_output=True)\n",
    "    return textrating\n",
    "\n",
    "#for length of comment\n",
    "def wordLengthGenerator(frame):\n",
    "    comments=frame.body\n",
    "    lengths=[]\n",
    "    for comment in comments:\n",
    "        lengths.append(len(comment))\n",
    "    return lengths\n",
    "\n",
    "#returns the ratio of uppercase to lower case letters\n",
    "#returns 0 if length is zero\n",
    "def upperCaseGenerator(frame):\n",
    "    comments=frame.body\n",
    "    ratios=[]\n",
    "    for comment in comments:\n",
    "        count = 0\n",
    "        for c in comment:\n",
    "            if(c.isupper()):\n",
    "                count = count + 1\n",
    "        if(len(comment)>0):\n",
    "            ratioval=float(count/len(comment))\n",
    "        else:\n",
    "            ratioval=0\n",
    "        ratios.append(ratioval)\n",
    "    return ratios\n",
    "\n",
    "#counts number of questions in a comment\n",
    "#by counting number of question marks\n",
    "def numberOfQuestionsInAComment(frame):\n",
    "    comments=frame.body\n",
    "    questions=[]\n",
    "    for comment in comments:\n",
    "        count=0\n",
    "        for c in comment:\n",
    "            if(c=='?'):\n",
    "                count=count+1\n",
    "        questions.append(count)\n",
    "    return questions\n",
    "\n",
    "\n",
    "#naive method of determining whether or\n",
    "#not the title is a question\n",
    "def isTitleAQuestion(frame):\n",
    "    START_WORDS = [\"who\", \"what\", \"when\", \"where\", \"why\", \"how\", \"is\", \"can\", \"does\", \"do\",\n",
    "                  \"could\",\"should\",\"would\",\"which\",\"whose\",\"whom\",\"are\"]\n",
    "    permalinks=frame.permalink\n",
    "    isQuestion=[]\n",
    "    for permalink in permalinks:\n",
    "        splitty=permalink.split('/')\n",
    "        title=splitty[5]\n",
    "        wordsInTitle=title.split('_')\n",
    "        firstWord=wordsInTitle[0]\n",
    "        if(firstWord in START_WORDS):\n",
    "            isQuestion.append(1)\n",
    "        else:\n",
    "            isQuestion.append(0)\n",
    "    \n",
    "    return isQuestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numberofQuestionsInAComment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-ce637ecab5aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwordLengthGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengineered_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0muppercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupperCaseGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengineered_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnumq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumberofQuestionsInAComment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengineered_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mistitleq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0misTitleAQuestion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengineered_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'numberofQuestionsInAComment' is not defined"
     ]
    }
   ],
   "source": [
    "readability= generatefkReadability(engineered_train)\n",
    "length=wordLengthGenerator(engineered_train)\n",
    "uppercase=upperCaseGenerator(engineered_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "numq=numberOfQuestionsInAComment(engineered_train)\n",
    "istitleq=isTitleAQuestion(engineered_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_contractions(frame):\n",
    "    #newthing=frame.copy()\n",
    "    contraction_list={\"arent\":\"aren't\",\"cant\":\"can't\",\"couldnt\":\"couldn't\",\"didnt\":\"didn't\",\"doesnt\":\"doesn't\",\n",
    "                 \"dont\":\"don't\",\"hadnt\":\"hadn't\",\"hasnt\":\"hasn't\",\"havent\":\"haven't\",\"hed\":\"he'd\",\"hes\":\"he's\",\n",
    "                 \"id\":\"i'd\",\"im\":\"i'm\",\"ive\":\"i've\",\"isnt\":\"isn't\",\"lets\":\"let's\",\"shouldnt\":\"shouldn't\",\"thats\":\"that's\",\n",
    "                 \"theres\":\"there's\",\"theyd\":\"they'd\",\"theyll\":\"they'll\",\"theyre\":\"they're\",\"theyve\":\"they've\",\"theyre\":\"they're\",\n",
    "                  \"wed\":\"we'd\",\"weve\":\"we've\",\"werent\":\"weren't\",\"whatll\":\"what'll\",\"whatre\":\"what're\",\"whats\":\"what's\",\n",
    "                  \"whatve\":\"what've\",\"wheres\":\"where's\",\"whos\":\"who's\",\"wholl\":\"who'll\",\"wont\":\"won't\",\"wouldnt\":\"wouldn't\",\n",
    "                  \"youd\":\"you'd\",\"youll\":\"you'll\",\"youre\":\"you're\",\"youve\":\"you've\"}\n",
    "    titles=[]\n",
    "    for fv in frame[\"post_title\"]:\n",
    "        titlewords=fv.split(' ')\n",
    "        twordnew=\"\"\n",
    "        for word in titlewords:\n",
    "            if word in contraction_list.keys():\n",
    "                newword=contraction_list[word]\n",
    "                twordnew+=(newword)+\" \"\n",
    "            else:\n",
    "                twordnew+= (word)+\" \"\n",
    "        titles.append(twordnew[:-1])\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nihilism across generations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7 years later i'm officially an us citizen murica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>took a awful joke seriously and now i'm in tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is this possible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in town for only a day what's the one place i ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                        nihilism across generations\n",
       "1  7 years later i'm officially an us citizen murica\n",
       "2  took a awful joke seriously and now i'm in tro...\n",
       "3                                   is this possible\n",
       "4  in town for only a day what's the one place i ..."
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titlesfix=fix_contractions(engineered_train)\n",
    "titlesfix=pd.DataFrame(titlesfix)\n",
    "titlesfix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['works', 'with', 'other', 'languages']\n"
     ]
    }
   ],
   "source": [
    "import wordsegment as ws\n",
    "ws.load()\n",
    "print(segment(ws.clean(\"workswithotherlanguages\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add\n",
    "titlesfix=pd.DataFrame(titlesfix) #segmented reddit titles\n",
    "istitleq=pd.DataFrame(istitleq) #is the post title a question\n",
    "numq=pd.DataFrame(numq) #number of questions in a comment\n",
    "readability=pd.DataFrame(readability) #readability score\n",
    "uppercase=pd.DataFrame(uppercase) #uppercase to lowercase ratio in post\n",
    "length=pd.DataFrame(length) #length of comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "11.0\n",
      "12.0\n",
      "13.0\n",
      "14.0\n",
      "15.0\n",
      "16.0\n",
      "17.0\n",
      "18.0\n",
      "19.0\n",
      "20.0\n",
      "21.0\n",
      "22.0\n",
      "23.0\n",
      "24.0\n",
      "25.0\n",
      "26.0\n",
      "27.0\n",
      "28.0\n",
      "29.0\n",
      "30.0\n",
      "31.0\n",
      "32.0\n",
      "33.0\n",
      "34.0\n",
      "35.0\n",
      "36.0\n",
      "37.0\n",
      "38.0\n",
      "39.0\n",
      "40.0\n",
      "41.0\n",
      "42.0\n",
      "43.0\n",
      "44.0\n",
      "45.0\n",
      "46.0\n",
      "47.0\n",
      "48.0\n",
      "49.0\n",
      "50.0\n",
      "51.0\n",
      "52.0\n",
      "53.0\n",
      "54.0\n",
      "55.0\n",
      "56.0\n",
      "57.0\n",
      "58.0\n",
      "59.0\n",
      "60.0\n",
      "61.0\n",
      "62.0\n",
      "63.0\n",
      "64.0\n",
      "65.0\n",
      "66.0\n",
      "67.0\n",
      "68.0\n",
      "69.0\n",
      "70.0\n",
      "71.0\n",
      "72.0\n",
      "73.0\n",
      "74.0\n",
      "75.0\n",
      "76.0\n",
      "77.0\n",
      "78.0\n",
      "79.0\n",
      "80.0\n",
      "81.0\n",
      "82.0\n",
      "83.0\n",
      "84.0\n",
      "85.0\n",
      "86.0\n",
      "87.0\n",
      "88.0\n",
      "89.0\n",
      "90.0\n",
      "91.0\n",
      "92.0\n",
      "93.0\n",
      "94.0\n",
      "95.0\n",
      "96.0\n",
      "97.0\n",
      "98.0\n",
      "99.0\n",
      "100.0\n",
      "101.0\n",
      "102.0\n",
      "103.0\n",
      "104.0\n",
      "105.0\n",
      "106.0\n",
      "107.0\n",
      "108.0\n",
      "109.0\n",
      "110.0\n",
      "111.0\n",
      "112.0\n",
      "113.0\n",
      "114.0\n",
      "115.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tum blr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>legal advice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gamestop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seattle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "0       tum blr\n",
       "1          pics\n",
       "2  legal advice\n",
       "3      gamestop\n",
       "4       seattle"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wordsegment import load, segment\n",
    "def segmentSubreddits(frame):\n",
    "    subreddits=frame['subreddit']\n",
    "    finalsb=[]\n",
    "    ws.load()\n",
    "    for i, sb in zip(range(len(subreddits)), subreddits):\n",
    "        if i%25000 == 0:\n",
    "            print(i/25000)\n",
    "        if len(sb) != 0:\n",
    "            segmented=segment(ws.clean(sb))\n",
    "            stringsb=\"\"\n",
    "            for c in segmented:\n",
    "                stringsb+=c+\" \"\n",
    "            finalsb.append(stringsb[:-1])\n",
    "        else:\n",
    "            finalsb.append(sb)\n",
    "    return finalsb\n",
    "\n",
    "subreddits_fixed=pd.DataFrame(segmentSubreddits(engineered_train))\n",
    "subreddits_fixed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_joblib_parallel_args'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-1beece082fa1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNearestCentroid\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseEnsemble\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mforest\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mforest\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mforest\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomTreesEmbedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseEnsemble\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_partition_estimators\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparallel_helper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_joblib_parallel_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_joblib_parallel_args'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import discriminant_analysis\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import ClusterCentroids \n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import coo_matrix, hstack, vstack\n",
    "\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "from itertools import compress\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn.utils import resample\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "path = './input/'\n",
    "#path = '../input/'\n",
    "data = pd.read_csv(path+'train.csv')\n",
    "submission_input=pd.DataFrame(enumerate(df['body'].tolist()))\n",
    "#submission_input = pd.DataFrame()    #pd.read_csv(path+'test.csv')\n",
    "print('Number of rows and columns in the train data set:',data.shape)\n",
    "print('Number of rows and columns in the test data set:',submission_input.shape)\n",
    "\n",
    "target_col = data.columns.values[range(2,8)]\n",
    "targets = data.iloc[:,range(2,8)]\n",
    "toxic_comment_data = data.loc[(targets==1).any(axis=1),:]\n",
    "\n",
    "class multi_labels_LogisticRegression:\n",
    "   \n",
    "    def __init__(self,penalty='l2', dual=False, tol=1e-4, C=1.0,\n",
    "                 fit_intercept=True, intercept_scaling=1, class_weight=None,\n",
    "                 random_state=None, solver='liblinear', max_iter=100,\n",
    "                 multi_class='ovr', verbose=0, warm_start=False, n_jobs=1,\n",
    "                min_df = 2, max_df = 0.95, n_gram = 4, n_features = 2000,\n",
    "                resampling = True, top_features = 1000,\n",
    "                all_comments = False,\n",
    "                num_class = 1, train_test_ratio = 0.8,\n",
    "                analyzers = ['char','word'],\n",
    "                fs = 'tree_50',\n",
    "                estimator = 'nn',\n",
    "                resampling_method = 'random',\n",
    "                preprocess = True,\n",
    "                fs_before_resampling = True):\n",
    "        \n",
    "        self.penalty = penalty\n",
    "        self.dual = dual\n",
    "        self.tol = tol\n",
    "        self.C = C\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.intercept_scaling = intercept_scaling\n",
    "        self.class_weight = class_weight\n",
    "        self.random_state = random_state\n",
    "        self.solver = solver\n",
    "        self.max_iter = max_iter\n",
    "        self.multi_class = multi_class\n",
    "        self.verbose = verbose\n",
    "        self.warm_start = warm_start\n",
    "        self.n_jobs = n_jobs\n",
    "        \n",
    "        #self.word_vectorizer = []\n",
    "        self.list_LogisticRegression = []\n",
    "        self.list_label = []\n",
    "        \n",
    "        #Probability predictions of Training and Testing\n",
    "        self.y_train_true = []\n",
    "        self.y_train_scores = []\n",
    "        self.y_test_true = []\n",
    "        self.y_test_scores = []\n",
    "        \n",
    "        self.word_vectorizers = []\n",
    "        #self.word_vectorizer_w = []\n",
    "        self.path = './input/' \n",
    "        self.target_col = []\n",
    "        self.list_topWord_Index = []\n",
    "        self.train_test_ratio = train_test_ratio\n",
    "        \n",
    "        #Score of Training and Testing\n",
    "        self.train_score = []\n",
    "        self.test_score = []\n",
    "        \n",
    "        #Model training scorer\n",
    "        self.train_scorer_ = []\n",
    "        self.results = []\n",
    "        \n",
    "        # Word matrix conversion factors\n",
    "        self.min_df = min_df\n",
    "        self.max_df = max_df\n",
    "        self.n_gram = n_gram\n",
    "        self.n_features = n_features\n",
    "        self.analyzers = analyzers\n",
    "        \n",
    "        self.resampling = resampling\n",
    "        self.resampling_method = resampling_method\n",
    "        self.top_features = top_features\n",
    "        \n",
    "        self.all_comments = all_comments\n",
    "        self.num_class = num_class\n",
    "        \n",
    "        self.fs = fs\n",
    "        self.estimator = estimator\n",
    "        \n",
    "        self.preprocess = preprocess\n",
    "        self.fs_before_resampling = fs_before_resampling\n",
    "        \n",
    "    \n",
    "    # Transforming comments list into word frequency matrix\n",
    "    #def data_transform(self, comments_list, vectorizers_list = self.word_vectorizers):\n",
    "    #    word_data_list = []\n",
    "    #    for i,word_vector in enumerate(vectorizers_list):\n",
    "    #        word_data_list = word_data_list + [word_vector.transform(comments_list)]\n",
    "    #    \n",
    "    #    return sparse.hstack(word_data_list)\n",
    "    \n",
    "    def data_transform(self, comments_list):\n",
    "        word_data_list = []\n",
    "        for i,word_vector in enumerate(self.word_vectorizers):\n",
    "            word_data_list = word_data_list + [word_vector.transform(comments_list)]\n",
    "        \n",
    "        return sparse.hstack(word_data_list)\n",
    "    \n",
    "    # Using Forest Trees to select top words\n",
    "    def features_selection(self, X,y):\n",
    "        \n",
    "        t0 = time()\n",
    "        if 'tree' in self.fs:\n",
    "            n = np.int(self.fs.split('_')[1])\n",
    "            print(\"Processing features selection by %d Forest Trees:...\" %n)\n",
    "\n",
    "            forest = ExtraTreesClassifier(n_estimators=n,\n",
    "                                  random_state=0, n_jobs=-1)\n",
    "\n",
    "            forest.fit(X,y)\n",
    "\n",
    "            #nb = MultinomialNB()\n",
    "            #nb.fit(X,y)\n",
    "\n",
    "            importances = forest.feature_importances_\n",
    "            std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                         axis=0)\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "            index_ = indices[range(self.top_features)]\n",
    "            \n",
    "        elif self.fs == 'sv':\n",
    "            svc = SVC(kernel=\"linear\")\n",
    "            print(\"Processing features selection by Kernel:...\")\n",
    "            # The \"accuracy\" scoring is proportional to the number of correct\n",
    "            # classifications\n",
    "            rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),\n",
    "                          scoring='accuracy', n_jobs=-1)\n",
    "            rfecv.fit(X, y)\n",
    "\n",
    "            print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "            # Plot number of features VS. cross-validation scores\n",
    "            plt.figure()\n",
    "            plt.xlabel(\"Number of features selected\")\n",
    "            plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "            plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "            plt.show()\n",
    "\n",
    "            index_ = rfecv.support_\n",
    "            print(\"Number of features selected by SV: %d\" %index_.sum())\n",
    "            \n",
    "        elif 'chi' in self.fs:\n",
    "            n = np.int(self.fs.split('_')[1])\n",
    "            print(\"Processing features selection by Chi2, k = %d:...\" %n)\n",
    "            ch2 = SelectKBest(chi2, k= n)\n",
    "            X_train = ch2.fit_transform(X, y)\n",
    "            \n",
    "            index_ = ch2.get_support(indices=False)\n",
    "            print(\"Number of features selected by Chi2: %d\" %index_.sum())\n",
    "            print(index_)\n",
    "        else:\n",
    "            index_ =  np.array([True]*X.shape[1])\n",
    "        \n",
    "        train_time = time() - t0\n",
    "        print(\"&\" * 80)\n",
    "        print(\"Features Selection time: %0.3fs\" % train_time)\n",
    "        \n",
    "        return index_\n",
    "    \n",
    "    # Resampling unbalanced dataset\n",
    "    def under_resampling_func(self, X, y):\n",
    "        t0 = time()\n",
    "        print(\"Processing Resampling...\")\n",
    "        \n",
    "        if 'random' in self.resampling_method:\n",
    "            rus = RandomUnderSampler(random_state=1)\n",
    "        else:\n",
    "            rus = ClusterCentroids(random_state=1, n_jobs=-1)\n",
    "        \n",
    "        X_res, y_res = rus.fit_sample(X, y)\n",
    "\n",
    "        train_time = time() - t0\n",
    "        print(\"Resampling time: %0.3fs\" % train_time)\n",
    "            \n",
    "        return X_res, y_res\n",
    "    \n",
    "    # Extracting dataset per \"Target\" class\n",
    "    def train_test_target_split(self, data, toxic_class, fraction = 0.8):\n",
    "        # Extracting data per \"toxic_class\"\n",
    "        is_toxic = data[data[toxic_class]==1]\n",
    "        no_toxic = data[~data.index.isin(is_toxic.index)]\n",
    "\n",
    "        # Shuffling the data before taking samples\n",
    "        is_toxic = shuffle(is_toxic)\n",
    "        no_toxic = shuffle(no_toxic)\n",
    "\n",
    "        ### Splitting data into Training and Testing\n",
    "\n",
    "        # Toxic data\n",
    "        training_toxic = is_toxic.sample(frac=fraction)\n",
    "        testing_toxic = is_toxic.loc[~is_toxic.index.isin(training_toxic.index)]\n",
    "\n",
    "        # Non-toxic data\n",
    "        training_no_toxic = no_toxic.sample(frac=fraction)\n",
    "        testing_no_toxic = no_toxic.loc[~no_toxic.index.isin(training_no_toxic.index)]\n",
    "\n",
    "        # Training and Testing datasets\n",
    "        data_training = shuffle(pd.concat([training_toxic, training_no_toxic], axis = 0))\n",
    "        data_testing = shuffle(pd.concat([testing_toxic, testing_no_toxic], axis = 0))\n",
    "\n",
    "        # Training and Testing 'comment_text' preditor, and target for \"toxic\" class\n",
    "        y_training_toxic = data_training[toxic_class]\n",
    "        y_testing_toxic = data_testing[toxic_class]\n",
    "\n",
    "        X_training_comment = data_training['comment_text']\n",
    "        X_testing_comment = data_testing['comment_text']\n",
    "        \n",
    "        # Generating Word_Vector for Training and Testing data\n",
    "        #print(\"Extracting features from the training data using a sparse vectorizer for class: %s\" %toxic_class)\n",
    "        X_train = self.data_transform(X_training_comment)\n",
    "        #print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "\n",
    "        #print(\"Extracting features from the test data using the same vectorizer for class: %s\" %toxic_class)\n",
    "        X_test = self.data_transform(X_testing_comment)\n",
    "        #print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "\n",
    "        # ***** Targets of Training and Testing datasets\n",
    "        y_train, y_test = y_training_toxic, y_testing_toxic\n",
    "\n",
    "        return {'X_train':X_train, 'y_train':y_train ,'X_test':X_test, 'y_test': y_test}\n",
    "    \n",
    "    # Transforming Comment dataset into Words matrix dataset\n",
    "    def words_matrix_convert(self, X, y):\n",
    "        \n",
    "        # Generating Word_Vector for Training and Testing data\n",
    "        print(\"Converting Comments dataset to Words Matrix\")\n",
    "        X_ = self.data_transform(X)\n",
    "        print(\"n_samples: %d, n_features: %d\" % X_.shape)\n",
    "\n",
    "        return X_, y\n",
    "    \n",
    "    def build_word_vectors(self, data):\n",
    "        t0 = time()\n",
    "        word_vectorizers = []\n",
    "        for analyzer in self.analyzers:    \n",
    "            print(\"Building words vector from the comments of the training data using a sparse vectorizer %s\" % analyzer)\n",
    "            word_vectorizer = TfidfVectorizer(min_df = self.min_df, max_df = self.max_df, lowercase=True, analyzer=analyzer,\n",
    "                            stop_words= 'english',ngram_range=(1,self.n_gram),max_features=self.n_features)\n",
    "            \n",
    "            if self.all_comments:\n",
    "                # Building word vector on the whole dataset\n",
    "                word_vector = word_vectorizer.fit_transform(data['comment_text'])#(data['comment_text'])\n",
    "            else:\n",
    "                targets = data.iloc[:,range(2,8)]\n",
    "                # Building word vector basing on Toxic comments only:\n",
    "                word_vector = word_vectorizer.fit_transform(data.loc[(targets==1).any(axis=1),:]['comment_text'])\n",
    "             \n",
    "            word_vectorizers = word_vectorizers + [word_vectorizer]\n",
    "\n",
    "        train_time = time() - t0\n",
    "        print(\"Word Matrix training time: %0.3fs\" % train_time)\n",
    "        \n",
    "        return word_vectorizers\n",
    "    \n",
    "    # Probability prediction\n",
    "    def toxic_predict(self, comments_list):\n",
    "        \n",
    "        if self.word_vectorizers == []:\n",
    "            print('The models are not yet trained!')\n",
    "        else:\n",
    "            X = self.data_transform(comments_list)\n",
    "        \n",
    "            y_pred = []\n",
    "            for i,col in enumerate(self.target_col):\n",
    "                \n",
    "                if self.top_features > 0:\n",
    "                    X_ = X.tocsc()[:,self.list_topWord_Index[i]]\n",
    "                else:\n",
    "                    X_ = X\n",
    "               \n",
    "                p = np.round(self.list_LogisticRegression[i].predict_proba(X_)[:,1],3)\n",
    "                y_pred = y_pred + [p]\n",
    "\n",
    "        return np.transpose(y_pred)\n",
    "    \n",
    "    def feature_selection_func(self, X_train, y_train, X_test, y_test):\n",
    "        if self.top_features > 0: \n",
    "            topWord_Index = self.features_selection(X_train, y_train)\n",
    "            self.list_topWord_Index = self.list_topWord_Index + [topWord_Index]\n",
    "            \n",
    "            X_train_, y_train_ = X_train.tocsc()[:,topWord_Index], y_train\n",
    "            print(\"Feature reduced X, n_samples: %d, n_features: %d\" % X_train_.shape)\n",
    "            \n",
    "            X_test_, y_test_ = X_test.tocsc()[:,topWord_Index], y_test\n",
    "        else:\n",
    "            X_train_, y_train_, X_test_, y_test_ = X_train, y_train, X_test, y_test\n",
    "        \n",
    "        return X_train_, y_train_, X_test_, y_test_\n",
    "    \n",
    "    def preprocess_function(self, X_train, y_train, X_test, y_test, fs_before_resampling = True):\n",
    "        \n",
    "        if fs_before_resampling:\n",
    "            X_train_, y_train_, X_test_, y_test_ = self.feature_selection_func(X_train, y_train, X_test, y_test)\n",
    "            if self.resampling: \n",
    "                X_train_, y_train_ = self.under_resampling_func(X_train_, y_train_)\n",
    "                print(\"Resampled X_train, n_samples: %d, n_features: %d\" % X_train_.shape)    \n",
    "            else:\n",
    "                print(\"No resampling...\")\n",
    "        else:\n",
    "            if self.resampling:\n",
    "                X_train_, y_train_ = self.under_resampling_func(X_train, y_train)\n",
    "                print(\"Resampled X_train, n_samples: %d, n_features: %d\" % X_train_.shape)\n",
    "            else:\n",
    "                print(\"No resampling...\")   \n",
    "            X_train_, y_train_, X_test_, y_test_ = self.feature_selection_func(X_train_, y_train_, X_test, y_test)\n",
    "            \n",
    "        return X_train_, y_train_, X_test_, y_test_\n",
    "    \n",
    "    def training_LogisticRegression(self, X_train, y_train, X_test, y_test): \n",
    "                                    #preprocess = True, fs_before_resampling = True):#, scores = ['recall']): #'precision']\n",
    "       \n",
    "        #### feature selection BEFORE RESAMPLING:\n",
    "        #if self.top_features > 0:\n",
    "            \n",
    "        #    topWord_Index = self.features_selection(X_train, y_train)\n",
    "        #    self.list_topWord_Index = self.list_topWord_Index + [topWord_Index]\n",
    "\n",
    "            # Reducing Features\n",
    "        #    X_train, y_train = X_train.tocsc()[:,topWord_Index], y_train\n",
    "        #    print(\"Feature reduced X_train, n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "\n",
    "        #    X_test, y_test = X_test.tocsc()[:,topWord_Index], y_test\n",
    "        #    print(\"Feature reduced X_test, n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "        \n",
    "        \n",
    "        #if self.resampling:\n",
    "        #   X_train, y_train = self.under_resampling_func(X_train, y_train)\n",
    "        #    print(\"Resampled X_train, n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "        #else:\n",
    "            #X_train, y_train = X_train, y_train\n",
    "        #    print(\"No resampling...\")\n",
    "        \n",
    "        \n",
    "        #### feature selection AFTER RESAMPLING:\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #######\n",
    "\n",
    "        t0 = time()\n",
    "        #for score in scores:\n",
    "        #print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        print(\"******** Beginning Training Process: %s *********\" %self.estimator)\n",
    "        #sv = GridSearchCV(SVC(), tuned_parameters, cv=5, scoring='%s_macro' % score)\n",
    "        scoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n",
    "        \n",
    "        if 'lr' in self.estimator:\n",
    "            ######Fitting Logistic Regression *********************\n",
    "            param_grid = {'C': [10, 20, 50, 100, 200, 500, 1000] }\n",
    "\n",
    "            sv = GridSearchCV(LogisticRegression(intercept_scaling=1,\n",
    "                        dual=False, fit_intercept=True, penalty='l2', tol=0.0001),\n",
    "                              param_grid = param_grid, n_jobs=-1, scoring=scoring, cv=10, refit='AUC')#, scoring='%s_macro' % score) # cv=5,\n",
    "        \n",
    "        elif 'sv' in self.estimator:\n",
    "            #### Fitting SVM*******************\n",
    "            parameters = {\n",
    "                'alpha': (0.00001, 0.000001),\n",
    "                'penalty': ('l2', 'elasticnet'),\n",
    "                'n_iter': (10, 50, 80),\n",
    "            }\n",
    "            sv = GridSearchCV(SGDClassifier(),\n",
    "                               n_jobs=-1, param_grid = parameters, scoring=scoring, cv=10, refit='AUC')\n",
    "        \n",
    "        else:\n",
    "            #self.estimator == 'nn':\n",
    "            #####Fitting CNN********************\n",
    "            params = {'hidden_layer_sizes': [(50,),(100,),(200,) ,(50,50,),(100,50,)]}\n",
    "            #params = {'hidden_layer_sizes': [(100,)],\n",
    "            #         'solver': ('lbfgs', 'adam')}\n",
    "            mlp = MLPClassifier()\n",
    "            sv = GridSearchCV(mlp, param_grid = params, verbose=10, n_jobs=-1, cv=5, scoring=scoring, refit='AUC')#\n",
    "        \n",
    "        sv.fit(X_train, y_train)\n",
    "        \n",
    "        train_time = time() - t0\n",
    "        print(\"********* TRAINING Time: %0.3fs\" % train_time)\n",
    "\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print()\n",
    "        print(sv.best_params_)\n",
    "        print()\n",
    "        \n",
    "\n",
    "        print(\"Detailed classification report:\")\n",
    "        print()\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are computed on the full evaluation set.\")\n",
    "        print()\n",
    "        y_true_test_sv, y_pred_test_sv = y_test, sv.predict(X_test)\n",
    "        print(classification_report(y_true_test_sv, y_pred_test_sv))\n",
    "        print()\n",
    "        print(\"Confusion matrix:\")\n",
    "        print(metrics.confusion_matrix(y_true_test_sv, y_pred_test_sv))\n",
    "            \n",
    "        return sv, X_train, y_train, X_test, y_test\n",
    "    \n",
    "    # Fitting the models\n",
    "    def fit(self, data):\n",
    "        \n",
    "        if self.word_vectorizers == []:\n",
    "            self.word_vectorizers = self.build_word_vectors(data)\n",
    "        \n",
    "        self.target_col = data.columns.values[2:2+self.num_class]\n",
    "        \n",
    "        for i,col in enumerate(self.target_col):\n",
    "            print('Building {} model for toxic class:{''}'.format(i,col)) \n",
    "    \n",
    "            model_input = self.train_test_target_split(data, col, fraction = self.train_test_ratio)\n",
    "\n",
    "            X_train, y_train  = model_input['X_train'], model_input['y_train'] \n",
    "            X_test, y_test = model_input['X_test'], model_input['y_test']\n",
    "            \n",
    "            print(\"FOR TRAINING - X_train, n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "            print(\"FOR TESTING: X_test, n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "            \n",
    "            if self.preprocess:\n",
    "                X_train, y_train, X_test, y_test = self.preprocess_function(X_train, y_train, \n",
    "                                                                            X_test, y_test, \n",
    "                                                                            self.fs_before_resampling)\n",
    "\n",
    "            sv, X_train, y_train, X_test, y_test = self.training_LogisticRegression(X_train, y_train, X_test, y_test)\n",
    "\n",
    "            self.list_label = self.list_label + [col]\n",
    "            self.list_LogisticRegression = self.list_LogisticRegression + [sv]\n",
    "            \n",
    "            self.y_train_true = self.y_train_true + [y_train]\n",
    "            self.y_train_scores = self.y_train_scores + [sv.predict_proba(X_train)]\n",
    "            \n",
    "            self.y_test_true = self.y_test_true + [y_test]\n",
    "            self.y_test_scores = self.y_test_scores + [sv.predict_proba(X_test)]\n",
    "            \n",
    "            self.train_score = self.train_score + [sv.score(X_train, y_train)]\n",
    "            self.test_score = self.test_score + [sv.score(X_test, y_test)]\n",
    "            \n",
    "            if self.estimator != 'nn':\n",
    "                self.train_scorer_ = self.train_scorer_ + [sv.scorer_]\n",
    "                self.results = self.results + [sv.cv_results_]\n",
    "\n",
    "            print('=' * 80)\n",
    "            \n",
    "    def print_roc_curve(self):\n",
    "        \n",
    "        for i,col in enumerate(self.target_col):\n",
    "            fpr_test, tpr_test, _ = roc_curve(self.y_test_true[i], self.y_test_scores[i][:,1])\n",
    "            fpr_train, tpr_train, _ = roc_curve(self.y_train_true[i], self.y_train_scores[i][:,1])\n",
    "\n",
    "            roc_auc_test = auc(fpr_test, tpr_test)\n",
    "            roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "            plt.figure()\n",
    "            lw = 2\n",
    "            plt.plot(fpr_test, tpr_test, color='darkorange',\n",
    "                     lw=lw, label='Test ROC curve (area = %0.2f)' % roc_auc_test)\n",
    "            plt.plot(fpr_train, tpr_train, color='blue',\n",
    "                     lw=lw, label='Train ROC curve (area = %0.2f)' % roc_auc_train)\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('ROC curve for toxic comment of label: {''}' .format(col))\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.show()\n",
    "    \n",
    "    def print_grid_search(self):\n",
    "        scoring, results = self.list_LogisticRegression[0].scorer_, self.list_LogisticRegression[0].cv_results_\n",
    "        \n",
    "        plt.figure(figsize=(13, 13))\n",
    "        plt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",\n",
    "                  fontsize=16)\n",
    "\n",
    "        plt.xlabel(\"C param\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.grid()\n",
    "\n",
    "        ax = plt.axes()\n",
    "        ax.set_xlim(0, 402)\n",
    "        ax.set_ylim(0.73, 1)\n",
    "\n",
    "        # Get the regular numpy array from the MaskedArray\n",
    "        X_axis = np.array(results['param_C'].data, dtype=float)\n",
    "\n",
    "        for scorer, color in zip(sorted(scoring), ['g', 'k']):\n",
    "            for sample, style in (('train', '--'), ('test', '-')):\n",
    "                sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n",
    "                sample_score_std = results['std_%s_%s' % (sample, scorer)]\n",
    "                ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n",
    "                                sample_score_mean + sample_score_std,\n",
    "                                alpha=0.1 if sample == 'test' else 0, color=color)\n",
    "                ax.plot(X_axis, sample_score_mean, style, color=color,\n",
    "                        alpha=1 if sample == 'test' else 0.7,\n",
    "                        label=\"%s (%s)\" % (scorer, sample))\n",
    "\n",
    "            best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n",
    "            best_score = results['mean_test_%s' % scorer][best_index]\n",
    "\n",
    "            # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "            ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n",
    "                    linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n",
    "\n",
    "            # Annotate the best score for that scorer\n",
    "            ax.annotate(\"%0.2f\" % best_score,\n",
    "                        (X_axis[best_index], best_score + 0.005))\n",
    "\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.grid('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def submission_output(self, submission_input):\n",
    "        Y_submit_pred = pd.DataFrame(self.toxic_predict(submission_input['comment_text']))\n",
    "        Y_submit_pred.columns = np.array(self.target_col)\n",
    "        submission_output = pd.concat([submission_input['id'], Y_submit_pred],axis=1)\n",
    "\n",
    "        return submission_output\n",
    "    \n",
    "    def to_submission_file(self, submission_output, filename = 'submission.csv'):\n",
    "        submission_output.to_csv(self.path+filename, index=False)\n",
    "        \n",
    "        #range(1), num_class = 1\n",
    "submission_data = []\n",
    "list_LR = []\n",
    "for i in range(1):\n",
    "    print(\"Training Process %s: \" %i)\n",
    "    print(\"%\"*80)\n",
    "    multi_labels_LR = multi_labels_LogisticRegression(min_df = 2, max_df = 0.95, n_gram = 5, n_features = 10000,\n",
    "                resampling = True, top_features = 5000,\n",
    "                all_comments = False,\n",
    "                num_class = 1, analyzers = ['char', 'word'], estimator = 'lr', fs = 'tree_25')\n",
    "    multi_labels_LR.fit(data)\n",
    "    submission_data = submission_data + [multi_labels_LR.submission_output(submission_input)]\n",
    "    list_LR = list_LR + [multi_labels_LR]\n",
    "    \n",
    "df_concat = pd.concat(submission_data)\n",
    "by_row_index = df_concat.groupby([df_concat.index, df_concat.id])\n",
    "df_means = by_row_index.mean()\n",
    "df_means = df_means.reset_index('id')\n",
    "\n",
    "toxic_comments=pd.DataFrame(df_means)\n",
    "#df_means.to_csv(path+'submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def build_dictionary(frame):\n",
    "    corpus=frame['body']\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer.get_feature_names()\n",
    "\n",
    "dict123=build_dictionary(engineered_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding fixed post title, whether title is a question, the number of questions in the comment, readability, ratio of uppercase to lowercase, and length of comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_train['post_title'] = titlesfix[0]\n",
    "engineered_train['title_is_question'] = istitleq[0]\n",
    "engineered_train['num_questions'] = numq[0]\n",
    "engineered_train['readability'] = readability[0]\n",
    "engineered_train['uppercase_ratio'] = uppercase[0]\n",
    "engineered_train['length'] = length[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subreddits_fixed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-5e4e528c3340>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mengineered_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'subreddit_sep'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubreddits_fixed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'subreddits_fixed' is not defined"
     ]
    }
   ],
   "source": [
    "engineered_train['subreddit_sep'] = subreddits_fixed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_corpus, common_texts, get_tmpfile\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "class EpochSaver(CallbackAny2Vec):\n",
    "\n",
    "    def __init__(self, path_prefix):\n",
    "        self.path_prefix = path_prefix\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "#         output_path = get_tmpfile('{}_epoch{}.model'.format(self.path_prefix, self.epoch))\n",
    "#         model.save(output_path)\n",
    "        model.save('newest_d2v_model.model')\n",
    "        self.epoch += 1\n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "        print(datetime.datetime.now())\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{} end\".format(self.epoch))\n",
    "        print(datetime.datetime.now())\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = Doc2Vec.load(\"final_d2v_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for i in range(len(d2v_model.docvecs)):\n",
    "    embeddings.append(d2v_model.docvecs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "11.0\n",
      "12.0\n",
      "13.0\n",
      "14.0\n",
      "15.0\n",
      "16.0\n",
      "17.0\n",
      "18.0\n",
      "19.0\n",
      "20.0\n",
      "21.0\n",
      "22.0\n",
      "23.0\n",
      "24.0\n",
      "25.0\n",
      "26.0\n",
      "27.0\n",
      "28.0\n",
      "29.0\n",
      "30.0\n",
      "31.0\n",
      "32.0\n",
      "33.0\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3062\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3063\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3064\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'd2v_100'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mset\u001b[1;34m(self, item, value, check)\u001b[0m\n\u001b[0;32m   4242\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4243\u001b[1;33m             \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4244\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3064\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3065\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'd2v_100'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-2bd4a4536de3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpositional_embedding\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcol_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"d2v_{0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0membedding_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositional_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0membedding_num\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0membedding_num\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3114\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3115\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3116\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3118\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3190\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3191\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3192\u001b[1;33m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3194\u001b[0m         \u001b[1;31m# check if we are modifying a copy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2596\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2597\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2598\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mset\u001b[1;34m(self, item, value, check)\u001b[0m\n\u001b[0;32m   4244\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4245\u001b[0m             \u001b[1;31m# This item wasn't present, just insert at end\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4246\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4247\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36minsert\u001b[1;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[0;32m   4372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4373\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4374\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4376\u001b[0m     def reindex_axis(self, new_index, axis, method=None, limit=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4102\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4103\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4104\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4105\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   5067\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5068\u001b[0m         merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n\u001b[1;32m-> 5069\u001b[1;33m                                       _can_consolidate=_can_consolidate)\n\u001b[0m\u001b[0;32m   5070\u001b[0m         \u001b[0mnew_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_blocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5071\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, _can_consolidate)\u001b[0m\n\u001b[0;32m   5090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5091\u001b[0m         \u001b[0margsort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5092\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5093\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5094\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding_df = pd.DataFrame()\n",
    "embedding_num = 0\n",
    "for positional_embedding in zip(*embeddings):\n",
    "    col_name = \"d2v_{0}\".format(embedding_num)\n",
    "    embedding_df[col_name] = pd.Series(positional_embedding)\n",
    "    embedding_num += 1\n",
    "    if embedding_num % 3 == 0:\n",
    "        print(embedding_num/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df.to_csv(\"d2v_train_doc_vecs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_d2v_model = Doc2Vec.load(\"newest_small_d2v_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_embeddings = []\n",
    "for i in range(len(small_d2v_model.docvecs)):\n",
    "    small_embeddings.append(small_d2v_model.docvecs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "11.0\n",
      "12.0\n",
      "13.0\n",
      "14.0\n",
      "15.0\n",
      "16.0\n"
     ]
    }
   ],
   "source": [
    "small_embedding_df = pd.DataFrame()\n",
    "embedding_num = 0\n",
    "for positional_embedding in zip(*small_embeddings):\n",
    "    col_name = \"d2v_{0}\".format(embedding_num)\n",
    "    small_embedding_df[col_name] = pd.Series(positional_embedding)\n",
    "    embedding_num += 1\n",
    "    if embedding_num % 3 == 0:\n",
    "        print(embedding_num/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_embedding_df.to_csv(\"small_d2v_train_doc_vecs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-5d9b9286311e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpost_title_similarity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mengineered_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mpost_title_similarity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimilarity_unseen_docs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'post_title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m25000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36miterrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    772\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m         \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 774\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    775\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mvalues\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4627\u001b[0m         \"\"\"\n\u001b[0;32m   4628\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4629\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AXIS_REVERSED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4630\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4631\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mas_array\u001b[1;34m(self, transpose, items)\u001b[0m\n\u001b[0;32m   3947\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3948\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3949\u001b[1;33m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interleave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3951\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtranspose\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_interleave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3958\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_interleaved_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3960\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3962\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "post_title_similarity = []\n",
    "for i, row in engineered_train.iterrows():\n",
    "    post_title_similarity.append(similarity_unseen_docs(model, row['body'], row['post_title'], alpha=0.1, min_alpha=0.0001, steps=5))\n",
    "    if i % 25000 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = {'author': str, 'author_flair_css_class': str, 'author_flair_text': str, 'body': str, 'can_gild': bool,\n",
    "              'controversiality': float, 'created_utc': int, 'distinguished': str, 'edited': bool, 'gilded': int,\n",
    "              'is_submitter': bool, 'permalink': str, 'score': int, 'stickied': bool, 'subreddit': str, 'subreddit_type':  str, \n",
    "              'num_outside_links': int, 'num_reddit_links': int, 'positive_sentiment': float, 'neutral_sentiment': float,\n",
    "              'negative_sentiment': float, 'compound_sentiment': float, 'named_entity_ratio': float, 'post_title': str, \n",
    "              'relevance_to_title': float, 'subreddit_mentions': int, 'user_mentions': int, 'has_flair': bool, 'links': str}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and drop unnecessary features before training them separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.drop(['author', author_flair_css_class', 'author_flair_text', 'body', 'permalink', 'subreddit'] <- for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_train.to_csv(\"engineered_reddit_train.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
